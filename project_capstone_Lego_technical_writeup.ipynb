{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks of Building Blocks\n",
    "## A tour of the Lego world in a data perspective\n",
    "\n",
    "\n",
    "### Executive Summary<a class=\"anchor\" id=\"Executive Summary\"></a>\n",
    "\n",
    "This is an ivestigation to explore the data related to Lego sets released since 1970. Lego started to manufacture its famous plastic bricks since the 1940s. From that time onwards numerous sets had been produced and sold worldwide. Lego has also accquired a massive fan base which share information and collections. This had opened up the prospect in perfoming analysis on Lego related data. This analysis can aid Lego collectors or retailers to make better decisions on the purchase or sale of Lego sets.\n",
    "\n",
    "The data is obtained by various means such as CSV files organised by individaul hobbyist. Web scraping and API enquiries were also used. EDA was performed and multiple visualisations were created to allow the readers to obtain a better understanding of the underlying data. Hypothesis testings were carried out to determine whether set with certain properties are different from the rest. \n",
    "\n",
    "One of the most important functions of data analysis is to make inferences or prediction using existing data. Hence predictive models were created and refined in order to predict the following:\n",
    "\n",
    "- Predict the Manufacturer Suggested Retail Price (MSRP)\n",
    "> MAE of 12.43 USD by Lasso regression with 100 PCs after PCA.\n",
    "- Distinguish desirable Lego sets\n",
    "> Accuracy of 95.7% by Gradient Boosting\n",
    "\n",
    "An article published by The Gurdian has stated that Lego could be a better investment when compared to gold or stocks. Hence a model was built to try distinguishing those Lego sets that beats the market. This has resulted in a classification accuracy of 77.9% Using an ensemble of various classifiers.\n",
    "\n",
    "The above work has set up the foundation for future investigations which are also suggested at the end of the report. These work includes the gather and utilisation of Lego pieces data which should provide an even more refined and precise into the predictive models. Image recognition techniques can also be used to further expand the project into a build recommendation system based on the Lego pieces that the user already pocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "[Executive Summary](#Executive Summary) <br />\n",
    "[Background](#Background) <br />\n",
    "[Problem Statement and Success Criteria](#Problem Statement and Success Criteria) <br />\n",
    "[Gathering and Cleaning Data](#Gathering and Cleaning Data)<br />\n",
    "[EDA and Visualisations](#EDA and Visualisations) <br />\n",
    "> [Unique Features](#Unique Features)<br />\n",
    "> [Year](#Year)<br />\n",
    "> [Themes](#Themes)<br />\n",
    "> [Subthemes](#Subthemes)<br />\n",
    "> [Minifigures](#Minifigures)<br />\n",
    "> [Manufacturer Suggested Retail Price (MSRP)](#Manufacturer Suggested Retail Price)<br />\n",
    "> [Packaging](#Packaging)<br />\n",
    "> [Availability](#Availability)<br />\n",
    "> [Wanted and Owned](#Wanted and Owned)<br />\n",
    "> [Ratings](#Ratings)<br />\n",
    "> [Theme Group](#Theme Group)<br />\n",
    "> [Numer of Reviews](#Nor)<br />\n",
    "> [Others](#Others)<br />\n",
    ">> [Effect of Inflation Adjustment on Prices](#Others)<br />\n",
    ">> [Price per Piece](#ppp)<br />\n",
    ">> [Popilarity and Cost](#p vs c)<br />\n",
    ">> [Relationship Between Features](#orbf)<br />\n",
    "\n",
    "[Clustering Analysis](#Clustering analysis) <br />\n",
    "> [K-Means](#K-Means) <br />\n",
    "> [DBScan](#DBScan) <br />\n",
    "> [Heierarchical Clustering](#Hierarchical Clustering) <br />\n",
    "\n",
    "[Hypothesis Testing](#Hypothesis Testing) <br />\n",
    "[MSRP Regression Analysis](#regression) <br />\n",
    "> [Lasso Grid Search](#lasso gscv) <br />\n",
    "> [Coefficient Analysis](#Coefficient Analysis) <br />\n",
    "> [PCA](#PCA) <br />\n",
    "> [Conclusion](#Conclusion R) <br />\n",
    "\n",
    "[Desirability Analysis (Classification)](#clf) <br />\n",
    "> [Feature Scaling for the Distance-Sensitive Models](#feature scaling) <br />\n",
    "> [Feature Importance](#feature importance) <br />\n",
    "> [Conclusion](#Conclusion C) <br />\n",
    "\n",
    "[Beating The Market](#Beating The Market) <br />\n",
    "> [Web Scraping](#Web Scraping)<br />\n",
    "> [Confirm Appreciation](#Confirm Appreciation)<br />\n",
    "> [XGBoost and Voting](#xgb)<br />\n",
    "> [Conclusion](#Conclusion B)<br />\n",
    "\n",
    "[Next Steps](#Next Steps) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In 1934, Lego was founded in Denmark by Ole Kirk Christiansen. Mr Christiansen started making wooden toys since 1932 and had produced different kinds of products. The predecessor of the today's famous plastic brick was launched in 1949 under the name \"Automatic Binding Bricks\". This was a risky move by Lego as it has spent  6.7% of its revenue on this machine alone. Luckily for Lego the investment paid off and Lego has since then progressed into one of the leading toy manufacturers in the world.\n",
    "\n",
    "Over the years Lego has improved and evolved many times to keep up with the market. The equivalents of the original bricks are still being manufactured but many more new themes have joined the lineup. For example Lego introduced the Duplo series was introduced in 1972. It is a series that are designed specially for children under the age of 5. These Duplo bricks features a significantly larger size which prevents them from chocking the target players. Another famous theme is the Techinic series which are models of much more complicated and sometimes machineries from the real world (for example BMW R 1200 GS Adventure Lego 42063). These advanced models are aim at more mature players. Modification to thes sets are also common which is partly due to the ease of integration of mechanical systems (such as pneumatic, electric and gears). Obtaining license from other products such as movies or comics has proved to be very successful for Lego as well. Amongst them the Star Wars theme is the most popular with a total of 585 sets released up to the date of writing. It can also be very collectable (expensive!?) with a Collector's  Millenium Falcon (Lego 10179) costing over 3000 GBP.\n",
    "\n",
    "Lego is also a big player in the robotics community. Due to the availability of parts and ease of constructions, Lego has been widely used in various robotic projects. In order to fulfill the demand from this sector, Lego released its own robotic package namely Mindstorms in 1998. Apart from the  various transducers and actuators, the series also features a programmable microchip called Intelligent Brick (similar to <a href='https://www.arduino.cc'>Arduino</a> and <a href='https://www.raspberrypi.org'>Raspberry Pi</a>). This makes Lego Mindstorms a very complete package for robotic hobbyists.\n",
    "\n",
    "There are many active communities formed by Lego enthusiasts. Some platforms are created specifically for trading such as <a href='http://www.bricklink.com'>Bricklink</a>. Other platforms are for Lego sets information such as <a href='http://www.brickset.com'>Brickset</a>. Some sites foucs more on Lego MOC which stands for \"My Own Creation\". MOC refers to non-official build ideas that are generated and shared amongst hobbyists. These information can be found in <a href='http://www.rebrickable.com'>Rebrickable</a>. Other great places to obtain infomation or generally discuss about Lego would be various groups and discussion forums such as <a href='https://www.reddit.com/r/lego/'>Reddit</a>.\n",
    "\n",
    "With the long history and a large fan base, Lego data has the potential in becoming an interesting field of study. One of the most popular investigations amongst enthusiast is the study carried out by <a href='http://www.realityprose.com/what-happened-with-lego/'>Reality Prose</a> where the relationship between Lego price and time was investigated. However it describes little about the pricing of Lego sets. It would be interesting for both retailers and hobbyist to find out what factors drive the price of Lego sets. It would also be useful for them to understand what might cause a Lego set to be desirable and collectable which in other words is a good investment. Therefore this project was carried out to tackle these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement and Success Criteria<a class=\"anchor\" id=\"Problem Statement and Success Criteria\"></a>\n",
    "\n",
    "\n",
    "Since the first creation of the famous plastic brick, Lego has expanded and developed many different lines and variations of bricks sets to provide more and better choice that suit different customers. It would be interesting for both retailers and hobbyist to find out what factors drive the price of Lego sets. It would also be useful for them to understand what might cause a Lego set to be desirable and collectable which in other words is a good investment. Some sources have stated that certain Lego sets are better investment than traditional investment products. Therefore it would also be interesting to find out and predict which Lego sets did/could likely beat the market. \n",
    "\n",
    "There are three major goals for this project. The first is to use various data and features to predict the MSRP of a Lego set. It is also important to determine the accuracy and methods for such predictions. This is because one can utilise either regression techniques or classification technique. The second goal would be to successfully come up with a metric to measure how collectable a Lego set is and be able to, again using the available data, predict which Lego sets are desirable for collectors. The third one will be to identify which Lego sets have beaten the market and to create a predictive model to try predicting sets that would beat the market in the future\n",
    "\n",
    "The success criteria for the project regarding the first goal is to be able to\n",
    "- Predict price of a Lego set within 15% of its manufacturer suggested retail price (MSRP). Although this number is rather vague, it serves as a solid first step of the investigation. \n",
    "\n",
    "The success criteria for the project regarding the second goal is to be able to\n",
    "- Define a metric to measure how desirable a Lego set is\n",
    "- Be able to classify Lego sets into more desirable/less desirable and beating the baseline accuracy (which depends on the definition of the metric)\n",
    "\n",
    "The success criteria for the project regarding the third goal is to be able to\n",
    "- Identify and validate Lego sets that have beaten the market\n",
    "- Be able to create a model that distinguishes Lego sets in their ability in beating the market and outperforms the baseline accuracy (which depends on the result of the identification step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering  and Cleaning Data<a class=\"anchor\" id=\"Gathering and Cleaning Data\"></a>\n",
    "\n",
    "The data for this project is gethered through 3 different channels. They are \n",
    "- CSV file from a Github Repository owned by a hobbyist <a href='https://github.com/seankross/lego/blob/master/data-tidy/legosets.csv'>seankross</a>\n",
    "- Application Programming Interface (API) provided by Brickset. The documentation can be found <a href='http://brickset.com/tools/webservices/v2'>here</a>\n",
    "- Web Scrapping to obtain information from Bricklink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The CSV file is downloaded into the local storage and the a series of procedures were applied to the data in order to clean and prepare data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "lego = pd.read_csv('assets/datasets/legosets.csv')\n",
    "lego.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/CSV_info.png'>\n",
    "\n",
    "It is apparent that there are quite a lot of missing values in the lego data frame. Therefore the first approach is to try filling in the missing values by accessing the API provided by Brickset.com. In the same time some more useful information will be recorded. These information include.\n",
    "- Number of members owing this item\n",
    "- Number of members wanting this item\n",
    "- Rating of the item in Brickset.com\n",
    "- Theme Group of item\n",
    "- Number of ratings in Brickset.com for the item\n",
    "\n",
    "Also it is possible to see that there are a lot of missing values in the MSRP columns. Therefore it would be good it check if any of the can be filled.\n",
    "\n",
    "Upon the examinations of the lego data, it is discoverd that some item numbers are repeated. For example Lego has a series of products called Collectable Minifigures. These minifigures are selaed in foil packs and sold seperately. Customers cannot see through the foil packs and this adds an element of excitment similar to lucky draws. However this means the different minifigures have the same item numbers which is indistinguishable. Fortunately it is discovered that in the Image_URL column, the exact Brickset item number can be found. Regular expressions are used to extract such information which is then stored in a new column called 'query_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(lego)):\n",
    "    try:\n",
    "        lego.ix[i,'query_id'] = re.search(r'images/(.+-\\d+)',lego.ix[i,'Image_URL']).group(1)\n",
    "    except: \n",
    "        print 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 new columns are created before the data is gathered. This is because it is considered to be a more effective way to achieve data gathering. Normally one would use the .apply() method on the 'query_id' column to extract information and directly assign to a new variable/column. However since this is only viable for 1 column each time and there are 5 pieces of information that needs to be obtained, it is decided that it would be easier to modify existing columns. This approach reduces the number of get requests performed which speeds up the process and reduce the loading on the server. The considerations here being that it is not the most ethical to hit a server too frequently in a short period of time and by doing so might result in an I.P address ban which is undesirable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego['wanted'] = 0\n",
    "lego['owned'] = 0\n",
    "lego['rating'] = 0\n",
    "lego['theme_grp'] = 0\n",
    "lego['review_num'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the get_info function to obtain the information. The method is rather simple by getting the information of certain Lego sets using the Brickset API. Then various functions are applied to extract the desired information by apply regular expression searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_info(x):\n",
    "    print x\n",
    "    URL = 'http://brickset.com/api/v2.asmx/getSets?apiKey=yn4G-wvgQ-wNct&userHash=&query=&theme=&subtheme=&setNumber='+x+'&year=&owned=&wanted=&orderBy=&pageSize=&pageNumber=&userName='\n",
    "    response = requests.get(URL)\n",
    "    res = response.content\n",
    "    get_wanted(x,res)\n",
    "    get_owned(x,res)\n",
    "    get_theme_grp(x,res)\n",
    "    get_rating(x,res)\n",
    "    get_review_num(x,res)\n",
    "    get_usd(x,res)\n",
    "    get_gbp(x,res)\n",
    "    get_cad(x,res)\n",
    "    get_eur(x,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wanted(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<wantedByTotal>(.+)</wantedByTotal>',res).group(1)\n",
    "    except:\n",
    "        x = '1'\n",
    "    lego.ix[lego['query_id'] == q, 'wanted'] = x\n",
    "def get_owned(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<ownedByTotal>(.+)</ownedByTotal>',res).group(1)\n",
    "    except:\n",
    "        x = '1'\n",
    "    lego.ix[lego['query_id'] == q, 'owned'] = x\n",
    "def get_theme_grp(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<themeGroup>(.+)</themeGroup>',res).group(1)\n",
    "    except:\n",
    "        x = 'not_applicable'\n",
    "    lego.ix[lego['query_id'] == q, 'theme_grp'] = x\n",
    "def get_rating(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<rating>(.+)</rating>',res).group(1)\n",
    "    except:\n",
    "        x = '0'\n",
    "    lego.ix[lego['query_id'] == q, 'rating'] = x\n",
    "def get_review_num(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<reviewCount>(.+)</reviewCount>',res).group(1)\n",
    "    except:\n",
    "        x = '0'\n",
    "    lego.ix[lego['query_id'] == q, 'review_num'] = x\n",
    "def get_usd(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<USRetailPrice>(.+)</USRetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'USD_MSRP'] = x\n",
    "def get_gbp(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<UKRetailPrice>(.+)</UKRetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'GBP_MSRP'] = x\n",
    "def get_cad(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<CARetailPrice>(.+)</CARetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'CAD_MSRP'] = x\n",
    "def get_eur(q,res):\n",
    "    try:\n",
    "        x = re.search(r'<EURetailPrice>(.+)</EURetailPrice>',res).group(1)\n",
    "    except:\n",
    "        x = np.nan\n",
    "    lego.ix[lego['query_id'] == q, 'EUR_MSRP'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.query_id.apply(get_info)\n",
    "lego.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/INFO_after_api.png'>\n",
    "\n",
    "The about info summary shows that there are still missing values in certain columns. However we now have the necessary columns to perform the analysis for the first 2 objectives. Since the API enquiry takes a rather large amount of time and computing power it is best to not performing it again. Hence a PostgreSQL database was created and the data stored in it as a table. Having gathered the required data the cleaning procedure would follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "local_engine = create_engine('postgresql://localhost:5432')\n",
    "conn = local_engine.connect()\n",
    "conn.execute(\"commit\")\n",
    "conn.execute(\"CREATE DATABASE capstone\")\n",
    "conn.close()\n",
    "engine_capstone=create_engine('postgresql://localhost/capstone')\n",
    "lego.to_sql('lego',engine_capstone,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lego sets can have more than 1 theme and this is denoted in the Subtheme column. However not everyset has a subtheme which is reflected in the data. However it would make analysis difficult if they are left to be 'NaN's hence they will be filled with the string 'No_Subtheme'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.Subtheme = lego.Subtheme.fillna('No_Subtheme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few instances with missing pieces information. It is because: \n",
    "- They are virtual products\n",
    "- Number of pieces are difficult to define\n",
    "- No information available\n",
    "\n",
    "And since these products make up a small part of the whole dataset it is decided they are to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego = lego[lego.Pieces.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lego are renouned for their huge variaty of minifigures. However not every sets include them. For example the Technic series which are models of various machines usually have no minifigures included. It is believed that the 'NaN' values in the minifigures column are actually 0s. Therefore they will be filled accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.Minifigures = lego.Minifigures.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 currencies for MSRP figures. They are USD, GBP, CAD and EUR. However as a target for regression analysis, we do not need that many. Therefore all values will be changed to USD for a better comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.reset_index(drop=True,inplace=True)\n",
    "for i in range(len(lego)):\n",
    "    usd = lego.ix[i,'USD_MSRP']\n",
    "    gbp = lego.ix[i,'GBP_MSRP']\n",
    "    gbp_usd = 1.27\n",
    "    cad = lego.ix[i,'CAD_MSRP']\n",
    "    cad_usd = 0.75\n",
    "    eur = lego.ix[i,'EUR_MSRP']\n",
    "    eur_usd = 1.07\n",
    "    if usd != None:\n",
    "        pass\n",
    "    elif (gbp!=None): \n",
    "        lego.ix[i,'USD_MSRP'] = float(gbp)*gbp_usd\n",
    "    elif (eur!=None): \n",
    "        lego.ix[i,'USD_MSRP'] = float(eur)*eur_usd\n",
    "    elif (cad!=None): \n",
    "        lego.ix[i,'USD_MSRP'] = float(cad)*cad_usd\n",
    "    else:\n",
    "        pass\n",
    "lego.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/after_fill_msrp.png'>\n",
    "\n",
    "By looping through the data frame most of this USD MSRP missing values are filled by converting from other currencies. However there are some that has no price information for every MSRP columns. Therefore they will have to be dropped. Together with them, the MSRP columns for GBP,CAD and EUR will also be dropped. The resulting dataframe would be stored in the database under the name lego_cleaned to avoid running through this cleaning procedure again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego = lego.drop(['GBP_MSRP','CAD_MSRP','EUR_MSRP'],axis=1)\n",
    "lego = lego.dropna()\n",
    "lego.to_sql('lego_cleaned',engine,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Visualisations<a class=\"anchor\" id=\"EDA and Visualisations\"></a>\n",
    "\n",
    "#### Unique Features<a class=\"anchor\" id=\"Unique Features\"></a>\n",
    "\n",
    "A brief description of the data can be made here. After the transfomations, there are 6060 rows and 20 columns left in the table. Some columns are unique and would add little value even if visulisations are applied. Those columns are \"Item_Number\", \"Name\", \"Image_URL\" and \"query_id\". \n",
    "\n",
    "#### Year<a class=\"anchor\" id=\"Year\"></a>\n",
    "\n",
    "The dataset contains records for Lego release fron 1975. It would be intresting to see the distribution amongst those years. It is also important to remember importing the required modules for the plotting tasks. Through out the report Matplotlib with Seaborn style plotting would be used as it provides the best felxibility in terms of plot s configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "yr = pd.pivot_table(lego, index=['Year'], values=['Name'], aggfunc='count')\n",
    "yr.plot(color='midnightblue', alpha=.7,ax=ax)\n",
    "ax.set(title='Number of Lego released by year', xlabel='Year', ylabel='Count',label='')\n",
    "ax.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/year_line.png'>\n",
    "\n",
    "The data set contains data ranges from 1971 to 2015. Although it is not the most updated (write up time of this experiment is Jan 2017), the amount of data (6060 entries at the moment) is sufficient for a proper analysis of trends and relationships. It can be seen that Lego has really rampped up their production through out the years. The most apparent drop in production would be around 2007 and 2008 when the financial crisis hit. It is worth noting that this plot does not show production number directly, it shows the variety of products released that year. However it is very likely for comapnies to expand their business in terms of production and variety in the same time when the economy is good. Therefore it would be a nice reference to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Themes<a class=\"anchor\" id=\"Themes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Theme.value_counts()[:20].plot(kind='barh', alpha=.7,ax=ax, color='midnightblue')\n",
    "ax.set(title='Top 20 Lego Theme', xlabel='Counts', ylabel='Theme')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lego_themes.png'>\n",
    "\n",
    "There are a total of 115 unique themes in the data set. It would be too much to display all of them. Therefore the above plot shows the top 20 themes in terms of numbers that were ever produced by Lego. It is worth noting that the top 20 themes have accounted for over 66% of all the data entries. This is an important finding as the large number of low frequency themes can introduce noise into the data which affects predictive modelling results.\n",
    "\n",
    "Having understand which themes have the most number of products, it would be good to see the number of years that the theme was actually in production. This is because naturally if the theme has been in production longer, it is more likely that it would have a higher number of products. If this is the case for every theme, the shape of the \"production years\" plot would be similar to the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theme_index = lego.Theme.value_counts()[:20].index.tolist()\n",
    "theme_index = [str(i) for i in theme_index]\n",
    "theme_pivot = pd.pivot_table(lego, index=['Theme'], values=['Year'], aggfunc=[np.min,np.max])\n",
    "theme_year = theme_pivot.loc[theme_index]\n",
    "c = theme_year.amax - theme_year.amin\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "c.plot(kind='barh', alpha=.7,ax=ax, color='crimson')\n",
    "ax.set(title='Years of production for the 20 top saling themes', xlabel='Years', ylabel='Theme')\n",
    "ax.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/theme_year.png'>\n",
    "\n",
    "It is immediately obvious that the shape of the plot is not as expected for every theme. Some themes have existed for long but the number of products are fewer than the other newer lines. For example the Space theme has been around for about 35 years but it has less products comparing to Collectable Minifigures. Duplo however has been the longest lasting theme yet as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subthemes<a class=\"anchor\" id=\"Subthemes\"></a>\n",
    "\n",
    "Many times a Lego set would have more than 1 theme. This is when the subtheme would come in to more specifically cataegorise the Lego set. For example the set Ferris Wheel 10247 (shown below), it has a theme of \"Advanced Model\" (persumably due to its complexity and large number of pieces) and a subtheme of Fairground. A similar approach can be taken in the analysis of sub themes.\n",
    "\n",
    "<img src='assets/pics/10247.jpg' style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Subtheme.value_counts()[1:21].plot(kind='barh', alpha=.7,ax=ax, color='midnightblue')\n",
    "ax.set(title='Top 20 Lego Sub Theme', xlabel='Counts', ylabel='Theme')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/subtheme.png'>\n",
    "\n",
    "From the above information we know that there are over 300 subthemes and each sub themes has less than 85 entries. Therefore each subtheme accounts for a very small part of the data set. The difference between sub themes are relatively small too. Therefore it is decided that the production years plot is not necessary. It is worth noting that it can be difficult to make sense with the subthemes alone. For example the subtheme that has the highest count is episode IV - VI. Without prior knowledge it would not be very interpretable (It turns out that this subtheme refers to Star Wars episode 4-6). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pieces <a class=\"anchor\" id=\"Pieces\"></a>\n",
    "\n",
    "The maximum number of pieces in a set up to year 2015 is 5922 and the minimum is 0. It is not reasonable for a set/item to have 0 pieces so those items would receive further investigations. The shape of the plot clearly demonstrates that the distribution is heavily skewed positively. With a median peice of 83, we know that over half of the data entries have pieces less than 100. In order to investigate deeper, a zoomed in version of the distribution will be plotted with a range of 0 - 500 pieces. A log transformed plot is also showed for a better comparison. A symmetric distribution is shown in the log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Pieces.hist(color='midnightblue', alpha=.7,ax=ax, bins=50)\n",
    "ax.set(title='Number of pieces', xlabel='Number of pieces', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pieces.png', style='width:50%; float:left;'><img src='assets/pics/log_piece.png', style='width:50%;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego[lego.Pieces<500]['Pieces'].hist(color='midnightblue', alpha=.7,ax=ax, bins=50)\n",
    "ax.set(title='Number of pieces', xlabel='Number of pieces', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/zoom_piece.png'>\n",
    "\n",
    "Again we can see that the distribution is still positively skewd. Median value is 63 and 75 percentile lies at 171. It is also realised that starting at about 300 pieces the variations in set counts with number of pieces reduces significantly.\n",
    "\n",
    "We then move on to the investigation of sets that contains 0 pieces. It is discovered that there are 2 sets with 0 pieces which have item numbers of 71009 and 8299 respectively. Item Number 71009 describes a whole set of Lego minifigures. Referencing similar sets such as Item Number 8833 the average number of piece per figure is 7. Therefore the Pieces column will be change to 16 * 7 = 112. Item Number 8299 has 377 pieces where data is obatined from this <a href='https://www.bricklink.com/v2/catalog/catalogitem.page?S=8299-1#T=P'>Bricklink</a> page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minifigures<a class=\"anchor\" id=\"Minifigures\"></a>\n",
    "\n",
    "Many sets contain minifigures to enhance the Lego experience. Minifigures can come with different bodys, legs and facial expressions. The different combinations add lots of interesting elements in the Lego range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.Minifigures.hist(color='midnightblue', alpha=.7,ax=ax, bins=np.linspace(0,32,17))\n",
    "ax.set(title='Number of Minifigures in a set', xlabel='Number of Minifigures', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/minifig.png'>\n",
    "\n",
    "Again the plot show positive skewness for number of Minifigures. At least 75% of the sets have less than 2 minifigures. However the maxmium number of Minifugres is 32. This is also possible becuase Lego would sometimes release sets that are focused in minifigures because some collectors focuses their collections on Minifigures. An example would be the Community Figures Set (9348) shown below.\n",
    "\n",
    "<img src='assets/pics/9348.png' style='width:50%;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manufacturer Suggested Retail Price (MSRP)<a class=\"anchor\" id=\"Manufacturer Suggested Retail Price\"></a>\n",
    "\n",
    "Similar to the plots for number of pieces, the price plots are also heavily skewed. Therefore a normal plot and zoomed in plot were created \n",
    "\n",
    "<img src='assets/pics/pirce.png' style='width:50%; float:left;'>\n",
    "<img src='assets/pics/price_zoom.png' style='width:50%;'>\n",
    "\n",
    "It is realised that most pieces are well below 20 USD. This is rather surprising as we can always see large sets in stores which costs a lot more. However 1 thing to bear in mind is that the prices in the data set has not been adjusted for inflation. Therefore inflation data is obtained from <a href='http://inflationdata.com/Inflation/Inflation_Rate/HistoricalInflation.aspx'>inflationdata.com</a> and adjustments were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inflation_lst = np.array([.043,.0327,.0616,.1103,.092,.0575,.065,.0762,.1122,.1358,.1035,.0616,.0322,.043,.0355,.0191,\n",
    "                 .0366,.0408,.0483,.0539,.0425,.0303,.0296,.0261,.0281,.0293,.0234,.0155,.0219,.0338,.0283,.0159,\n",
    "                 .0227,.0268,.0339,.0324,.0285,.0385,-0.0034,.0164,.0316,.0207,.0147,.0162])\n",
    "inf_rev = 1 - inflation_lst\n",
    "inf = [np.prod(inf_rev[i:]) for i in range(len(inflation_lst))]\n",
    "def adj_coef(x):\n",
    "    if x != 2015:\n",
    "        diff = x-1971\n",
    "        return inf[diff]\n",
    "    return 1\n",
    "lego['adjust_coef'] = lego['Year'].apply(adj_coef)\n",
    "lego.USD_MSRP = lego.USD_MSRP.astype(float)\n",
    "lego['adj_USD_MSRP'] = lego.USD_MSRP/lego.adjust_coef\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.adj_USD_MSRP.astype(float).hist(color='crimson', alpha=.7,ax=ax, bins=50)\n",
    "ax.set(title='Lego sets MSRP (USD)', xlabel='MSRP (USD)', ylabel='Count')\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "lego.ix[lego.adj_USD_MSRP<100,'adj_USD_MSRP'].astype(float).hist(color='crimson', alpha=.7,ax=ax, bins=20)\n",
    "ax.set(title='Lego sets MSRP (USD)', xlabel='MSRP (USD)', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/adj_price.png' style='width:50%; float:left;'>\n",
    "<img src='assets/pics/adj_price_zoom.png' style='width:50%;'>\n",
    "\n",
    "\n",
    "Although the skewness is still there (which is expected), the distribution is now more 'even'. There are significantly more sets that ranges from 5 USD to 15 USD in terms of ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packaging<a class=\"anchor\" id=\"Packaging\"></a>\n",
    "\n",
    "Most Lego sets are packed as we see them - in boxes. However there are also other packaging methods. Although not as commonly seen as boxes, packaging using plastic bags are also popular. Those are called Polybags. In fact the inclusion of polybags in the data set might be one of the causes of skewness in price and pieces as polybags are usually samller in size (and hence a somewhat lower price is expected). Foilbags are used when Lego would like to hide the content in the bag from customers. The most popular example would be the Collectable Minifigure Series. It should also be noted that there are over 1500 instances with no packaging information.\n",
    "\n",
    "<img src='assets/pics/packaging.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Availability<a class=\"anchor\" id=\"Availability\"></a>\n",
    "\n",
    "Most Lego sets are available for normal retail. However there are also other channels that Lego distributes its products. For example there are Legoland exclusive sets that customers can only purchase in Legoland. There are also limited editions that availability is significantly less than normal products. Similar to the Packaging feature, the availability feature has got over 1500 not-specified instances.\n",
    "\n",
    "<img src='assets/pics/availability.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanted and Owned <a class=\"anchor\" id=\"Wanted and Owned\"></a>\n",
    "\n",
    "In the website Brickset.com, registered users are able to indicate their interest in specific Lego sets by clicking a button saying that they want it. They can also record the ownership of their own Lego sets. Therefore for the same Lego item number, there are two figures showing the 'want' and 'own' respectively. The following plots demonstrate the distribution of such figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego.wanted = lego.wanted.astype(float)\n",
    "lego.owned = lego.owned.astype(float)\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "lego.owned.hist(color='midnightblue', alpha=.7,ax=ax1, bins=20)\n",
    "ax1.set(title='Lego ownership in Brickset', xlabel='Number Owned', ylabel='Counts')\n",
    "lego.wanted.hist(color='midnightblue', alpha=.7,ax=ax2, bins=20)\n",
    "ax2.set(title='Lego wanted by members in Brickset', xlabel='Number Wanted', ylabel='Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ownwant.png'>\n",
    "\n",
    "From the plots above it can be seen that the distributions (in terms of shape) are very similar. Most pieces have very little votes. This could be due to the fact that not many sets are high profile sets and the majority receives very little attention. Therefore their votes on either categories are relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(lego.owned,lego.wanted, color='midnightblue', alpha=.7, label='data')\n",
    "ax1.plot(lego.owned,lego.owned, color='crimson', alpha=.7, label = 'Owned = Wanted')\n",
    "ax1.set(title='Owned vs Wanted', xlabel='Owned', ylabel='Wanted',xlim=[0,16000],ylim=[0,10000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ownscatter.png'>\n",
    "\n",
    "The above scatter plot describes the relationship between the 'wants' and 'owns' for various Lego sets. In order to assist the interpretation of the plot, a line showing want = own is also plotted. As expected we can see from the graph that most data points fall under the line. This means that most sets have more owns than wants. This is very common as the simple economic theory suggest that the rarer an items is the less supply it has; which in turns leads to a small number of total owners. The top 3 wanted sets, which all of them have over 8000 wanted votes, are shown below (2 of the are from Modular Building Series and the other is the Collector's Millenium Falcon): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/10182.jpg' style='width:30%; float:left;'>\n",
    "\n",
    "<img src='assets/pics/10185.png' style='width:38%; float:right;'>\n",
    "<img src='assets/pics/10179.jpg' style='width:28%; float:right;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More on the definition of classification labels<a class=\"anchor\" id=\"label definition\"></a>\n",
    "\n",
    "As we can tell from the data set that there are no obvious labels to differentiate common sets from desirable sets, it is important to discuss how 'desirable-ness' should be measured. In the following summary the considered metrics would be proposed and its relevant pros and cons will be discussed.\n",
    "\n",
    "- Using the 'Wanted' number:\n",
    "This is the most straight forward metric that one can derived at. The biggest advantage in using this metric is that it is simple and easy to understand. The disadvantage of simply using 'wanted' is that it might not capture some information that is more complicated such as the number of people wanting a specific set. In other words if many people want a certain set but in the same time even more people own it. Does that still make the set desirable/collectable?\n",
    "\n",
    "\n",
    "- Using $Wanted/Owned$:\n",
    "Addressing the problem discussed above, one can use another label metric (denoted l): $$l = \\frac{Wanted}{Owned}$$This metric takes into account of set ownership as well. That is if a highly wanted set is also owned by many, l would not be high. This can in a way more accurately reflect the rarity of a Lego set. However this leads to another potential problem. The same 'l' can be achieved by different combinations of wanteds and owneds. For example having a 'l' metric of 3, there could be 3000 wanted and 1000 owned for set A and 3 wanted and 1 owned for set B. Common sense tells us that set A and set B are probably not equally desirable. Since set B receives less 'votes' in each category it might just means that set B is not very popular so hobbyist do not bother expressing their interest/ownership. This effect is not reflected in this 'l' metric.\n",
    "\n",
    "- Incorpration of certain 'distance' components:\n",
    "One way to mitigate the problem mentioned above is to include a distance component in the 'l' metric. There are two major ways that this could be achieved. The first one is to include a component in proportion to the data point's distance from the origin (point 0,0). This distance can effectively reflect the popularity of a data point. The greater the distance, the more vote a set has. This is because such distance is the square root of 'owned' squared + 'wanted' squared. \n",
    "\n",
    "Another distance component is the normal distance from the line wanted = owned. Since the data points can not have negative values, this distance is inheritedly small when it is near the origin. \n",
    "\n",
    "However it is tricky to incorprate these distance components in the 'l' metric. It is difficult to properly quantify their weighting and their effect on the metric. It is also more difficult for audiences to comprehend this metric.\n",
    "\n",
    "\n",
    "From the discussion above we know that there are advantages in itroducing a more complex lable to the dataset. However it is difficult to incorporate the metric properly. This is due to the fact that it is not poosible to easily integrate the distance component nor is the explantion to target audiences. Therefore for now the 'wanted' column wills serve as the 'desirable-ness' label with the top 10% defined as 'desirable'. However other labeling methods will also be investigated and improvement in this labeling system can take place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratings <a class=\"anchor\" id=\"Ratings\"></a>\n",
    "\n",
    "The shape of the rating distribution is rather unexpected. It can be seen that the ratings are concentrated in either end of the plot. It is suspected that most of the 0 rated items are actually items without ratings and the values were filled in during the data cleaning process. At the other end of the spectrum lies the review of the users. It is common for hoobyist to give reviews to those sets that they like but not to criticise the sets that are average. This could be the reason leading to this extreme situation.\n",
    "<img src='assets/pics/rating.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theme Group<a class=\"anchor\" id=\"Theme Group\"></a>\n",
    "\n",
    "Theme group is just like \"Theme\" or \"Subtheme\" where sets are categorised. However theme group has less overall categories which allows users to compare better. It seems that theme group tends to indicate the 'purpose' of the set more than the actual background theme of the set. \n",
    "\n",
    "<img src='assets/pics/theme_grp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Reviews<a class=\"anchor\" id=\"Nor\"></a>\n",
    "\n",
    "Again it shows that most sets have very small number of reviews which is less than or equal to 2. It is suspected that because most hobbyist would like to reviews sets that are special or popular - which accounts for a very small number of the total available data. The maximum review number is 97 and the minimum is 0. The median value is 2.\n",
    "\n",
    "<img src='assets/pics/review_num.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others<a class=\"anchor\" id=\"Others\"></a>\n",
    "\n",
    "##### Effect of Inflation Adjustment on Prices\n",
    "\n",
    "The plots show the effect of inflation adjustment. Although the distribution shape stays rather similar, some patterns or evidence or thresholds disappear which were shown as horizontal lines in the non-adjusted plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.scatter(lego.Pieces,lego.adj_USD_MSRP, color='midnightblue', alpha=.7)\n",
    "ax1.set(title='adj_USD', xlabel='Number of pieces', ylabel='adj_USD')\n",
    "ax2.scatter(lego.Pieces,lego.USD_MSRP, color='midnightblue', alpha=.7)\n",
    "ax2.set(title='USD', xlabel='Number of pieces', ylabel='USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/adj_effect.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Price per Pieces  <a class=\"anchor\" id=\"ppp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppp = lego.adj_USD_MSRP/lego.Pieces\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ppp.hist(color='midnightblue', alpha=.7,ax=ax, bins=20)\n",
    "ax.set(title='Price per piece distribution', xlabel='Price per piece', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ppp.png'>\n",
    "\n",
    "From the above plot it can be seen that the price per pieces figures are mostly under 0.4 (0.37 is the 75 percentile). However some sets have a figure as high as over 207.30. In order to understand what is going on there a futher investigation was carried out.\n",
    "\n",
    "By looking at the data sorted by price per piece in decending order, it is possible conclude that the high 'price per piece' cases comes from mostly the Mindstorms theme. As mentioned in the background section, Mindstorms is a series of components made by Lego to allow users build complex mechanical systems. It is very widely used in robotics as we can see there are various sensors and transducers included in the theme. Due to the complexity the price of each piece would naturally be very high comparing to a plain plastic brick. Other unusually high priced pieces includes a play wall in the Duplo series which is like a table where you can build Lego on top. The following pivot table reinforces the statement made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(lego, index=['Theme'], values=['Price_per_piece']).sort_values(by='Price_per_piece',ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ppp_sort.png'>\n",
    "\n",
    "The 2 top infintie values are likely caused by pieces being 0 for some entries for that category. This could generate error down the road but those will be dealt with in the modelling section. From the table it can be seen that after the errors, Mindstorms has and average of 37.4 USD per piece which is followed by Power Function (which also includes parts such as motors) in its series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Popularity and Costs <a class=\"anchor\" id=\"p vs c\"></a>\n",
    "\n",
    "It would be normal to assume that when Lego recognises that some sets/themes tend to be more popular, it would give those sets a marked up price. Hence a 'wanted' vs 'Price_per_piece' scatter plot was generated to examine such effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(lego.wanted,lego.Price_per_piece,color='midnightblue', alpha=.7)\n",
    "ax.set(title='Popularity vs Price per pieces', xlabel='Number of wants', ylabel='Price per pieces', ylim=[0,50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/w_vs_ppp.png'>\n",
    "\n",
    "However from the plot there are no obvious trends demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of Relationship Between Features<a class=\"anchor\" id=\"orbf\"></a>\n",
    "\n",
    "In order to gain a deeper understanding in the relationship between features, a heatmap is created. The deeper the color of a square(either blue or red) the greater the realtionship there is. Red represents a positive crrelation and blue represents a negative correlation. From the plot we can see that there is a diagonal line runing thorguh the center which means variables are 100% correlated with themselves (obviously). However there are also other deep solid red squares within the map. These pairs are\n",
    "\n",
    "- Number of pieces and adjusted price\n",
    "- Number of 'wanted' votes and number of 'owned' vote\n",
    "- Number of 'wanted' votes and number of pieces\n",
    "\n",
    "These are all paris of variables that positively correlate with each other. In order to examine the actual distribution, a pairplot is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.heatmap(lego.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/heat_map.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lego_model = lego.drop(['Theme','Subtheme','Packaging','Availability','theme_grp'],axis=1)\n",
    "lego_model.rating = lego_model.rating.astype(float)\n",
    "lego_model.review_num = lego_model.review_num.astype(float)\n",
    "sns.pairplot(lego_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pairplot.png'>\n",
    "\n",
    "The pairplot shows various scatter plots and histograms to demonstrate relationship between features. In order to examine more on the findings that we obtained in the heat map, the relevant plots are analysed.\n",
    "\n",
    "We can see that all the combinations with higher correlations have a shape more like a funnel. Although it seems to be quite spreaded out, we have to remeber that we have more than 6000 data points and many of them might be overlayed by others. If the overlayed ones lie very close together and forms a linear line, the correlation coefficient can still be high. \n",
    "\n",
    "On the other hand others plots have either an irregular shape or shaped like a cluster that grows equally from the origin.\n",
    "\n",
    "There are other interesting insights that might or might not help in the modelling phase:\n",
    "\n",
    "\n",
    "- More recent sets are more popular both in terms of 'wanted' and 'owned'\n",
    "- Sets released in 2008 and 2009 seems to be reviewed the most which we can assume that they are more popular\n",
    "- Lego sets get bigger and bigger through out the year and topped at around 2008 and 2009 \n",
    "- However the most reviewed sets are not the largest sets. The most reviewed sets all have a number of pieces under 2000\n",
    "- There are usually Minifigures in sets that get reviewed a lot\n",
    "- Highly wanted sets are not necessarily expensive (vice versa)\n",
    "- A minimum rating of 4 is almost guaranteed if a set has at least 60 reviews\n",
    "- The number of reviews decreases as the sets get more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering analysis<a class=\"anchor\" id=\"Clustering analysis\"></a>\n",
    "\n",
    "It is a common practise for data scientists to perform unsupervised learning (clustering) analysis during the EDA phase. The aim of clustering analysis is to organise similar data points together into groups (clusters). This can be especially helpful when the feature space is large and complicated where relationships cannot be discovered easily. Clustering analysis can also give an overview of the underlying data structure which can help a data sceintis to gauge his expectations in other models.\n",
    "\n",
    "Before performing the clustering analysis, it is important to understand what the algorithm is doing and make adjustments to the data accordingly. For many clustering algorithm, the Eucledian distances between data points are often used. This implies that feature scaling has to be carried out to prevent some features from dominating. In this example it would probably be the 'owned' feature which has the largest inter-feature variance.\n",
    "\n",
    "#### K-Means<a class=\"anchor\" id=\"K-Means\"></a>\n",
    "\n",
    "K-means clustering is an algorithm that requires the user defining the number of clusters. Therefore 4 different number of clusters are experimented and the results are shown below. It is obviously shown in the plots that there are no obvious clusters visible (which we can tell from the pairplot above). However it seems that K-means was able to differentiate groups of high-owned,low-wanted and high-wanted,low-owned (clusters in purple and yellow respectively in the 5-clusters plot). It can also distinguish relatively unpopular sets i.e. low-owned,low-wanted (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "lego_std = StandardScaler().fit_transform(lego)\n",
    "label_lst = []\n",
    "for n in range(2,6):\n",
    "    clus = KMeans(n_clusters=n,random_state=42).fit(lego_std)\n",
    "    label_lst.append(clus.labels_)\n",
    "fig = plt.figure(figsize=(18,14))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax1.scatter(lego.wanted, lego.owned, c=label_lst[0], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax2.scatter(lego.wanted, lego.owned, c=label_lst[1], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax3.scatter(lego.wanted, lego.owned, c=label_lst[2], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax4.scatter(lego.wanted, lego.owned, c=label_lst[3], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax1.set(xlabel='Wants', ylabel='Owns', title='2 groups clustering using KMeans')\n",
    "ax2.set(xlabel='Wants', ylabel='Owns', title='3 groups clustering using KMeans')\n",
    "ax3.set(xlabel='Wants', ylabel='Owns', title='4 groups clustering using KMeans')\n",
    "ax4.set(xlabel='Wants', ylabel='Owns', title='5 groups clustering using KMeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/kmeans.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan<a class=\"anchor\" id=\"DBScan\"></a>\n",
    "\n",
    "Using DBScan, which is another type of clustering method, has yielded a very different result. Since number of clusters are not fixed in DBScan, the results are solely determined by the hyperparameters used in fitting the model. For DBScan it would be the 'eps' (the distance within which can nearby data form a cluster) and 'min_samples' (the minimum samples required for a cluster to form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "lego_std = StandardScaler().fit_transform(lego)\n",
    "clus = DBSCAN(min_samples=6, eps=1).fit(lego_std)\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(lego.wanted, lego.owned, c=clus.labels_, cmap=plt.get_cmap('rainbow'), alpha=.4)\n",
    "ax.set(xlabel='Wants', ylabel='Owns', title='Clustering using DBScan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/dbscan.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering<a class=\"anchor\" id=\"Hierarchical Clustering\"></a>\n",
    "\n",
    "Hierarchical clustering is yet anther method for performing clustering. It tries to connect the nearest data points together in each iteration until all the data points are connected. Once a group was formed, its 'location' is represented by the group's centroid. A Dendogram can be plotted to demonstrate how all the data connects however in our case there are too many data points which can not be displayed properly on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Z = linkage(lego, 'ward')\n",
    "c, coph_dists = cophenet(Z, pdist(lego))\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.title('Dendrogram',fontsize=50)\n",
    "plt.xlabel('Sets',fontsize=30)\n",
    "plt.ylabel('Distance',fontsize=30)\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=0.,  \n",
    "    leaf_font_size=18.,\n",
    ")\n",
    "plt.yticks(fontsize=25.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/dendogram.png'>\n",
    "\n",
    "The number of clusters produced in a hierachical clustering analysis depends on the decision of the analyst. The analyst can decide on a cut off point by looking at the 'Distance' axis. For example in our case a cut off of 60 would produce 5 clusters while a cut off at 125 would produce 2 clusters. Then clustering plots like the ones above can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_d = [125,100,80,60]\n",
    "clus_lst =[]\n",
    "for md in max_d:\n",
    "    clusters = fcluster(Z, md, criterion='distance')\n",
    "    clus_lst.append(clusters)\n",
    "fig = plt.figure(figsize=(18,14))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax1.scatter(lego.wanted, lego.owned, c=clus_lst[0], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax2.scatter(lego.wanted, lego.owned, c=clus_lst[1], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax3.scatter(lego.wanted, lego.owned, c=clus_lst[2], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax4.scatter(lego.wanted, lego.owned, c=clus_lst[3], cmap=plt.get_cmap('rainbow'), alpha=.3)\n",
    "ax1.set(xlabel='Wants', ylabel='Owns', title='2 clusters @ distance = 125')\n",
    "ax2.set(xlabel='Wants', ylabel='Owns', title='3 clusters @ distance = 10')\n",
    "ax3.set(xlabel='Wants', ylabel='Owns', title='4 clusters @ distance = 80')\n",
    "ax4.set(xlabel='Wants', ylabel='Owns', title='5 clusters @ distance = 60')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/hier_clus.png'>\n",
    "\n",
    "The plots above show results that are reasonably similar to those produced by K-means. Similarly with the 4 cluster groups it seems that hierarchical clustering can can group data using number of wants and owns. The cluster boundaries are very similar to K-means as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing<a class=\"anchor\" id=\"Hypothesis Testing\"></a>\n",
    "\n",
    "There are various Lego themes available in the market. It is also always an idea that some themes are more popular/expensive than the others. In this investigation, the aim is to verify this assumption with some of the better known themes such as Star Wars or Marvel Super Heros. Hypothesis testing will be carried out to compare the 'price per piece' of Lego sets to determine if Lego prices their sets differently. A subset of the complete dataset is used because the complete set contains many data that can seriously affect the test results. For example the Mindstorms series that we came across previously is usually very expensive as they consists of various sensors and motrs (or even a programmable chip). Therefore it is best to exclude those special cases to ensure we are comparing apples to apples (or bricks to bricks).\n",
    "\n",
    "Normally when hypothesis tests are mentioned, we think of t-tests or z-tests. These are the most common test seen in various situations. However in this case those tests are not appropriate because one of the prerequisite assumptions is not be met. This is namely the assumption normality of data which we found out during earlier analysis. From the histograms above we know that 'price per piece' for either of the groups are not normally distributed so t-test or z-test are not applicable. To solve this problem a Mann-Whitney test will be performed as this test is more robust to data with different distributions. One of the major reasons being that Mann-Whitney test utilises the value ranks of each data points instead of the mean in the test.\n",
    "\n",
    "4 groups are picked to be compared to the population. These are all common themes that we can see in toy stores. Like a t-test, a null hypothesis is defined. The null hypotheis suggests that there are no difference in price per piece between the population and the selected group. It would be reasonable to set a 95% confidence interval as this is just a normal hypothesis test which the result would not cause an immediate huge imapact (unlike those of mdicine testings). Therefore it would require a p-value of 0.05 or lower to reject the null hypothesis. Initial plots are shown to get an idea of what each group's distribution is like.\n",
    "\n",
    "<img src='assets/pics/4grp_dist.png'>\n",
    "\n",
    "It can be seen that for most groups the prices per piece are below 1. However the Star Wars theme has a few outliers that are very differnt. It might cause the test result to suggest it is different from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "lego['Price_per_piece'] = lego.adj_USD_MSRP/lego.Pieces\n",
    "relavent_lst = ['Duplo','Star Wars','City','Classic','Creator','The Simpsons','Scooby-Doo','Architecture','Elves',\n",
    "                'Friends','Ideas','Juniors','Minecraft','Ninjago','DC Comics Super Heroes','Marvel Super Heroes',\n",
    "                'Technic']\n",
    "condensed_lego = lego[lego['Theme'].isin(relavent_lst)]\n",
    "condensed_lego = condensed_lego[condensed_lego['Price_per_piece']!=0]\n",
    "\n",
    "\n",
    "population = condensed_lego['Price_per_piece']\n",
    "sw = condensed_lego[condensed_lego['Theme']=='Star Wars']['Price_per_piece']\n",
    "msh = condensed_lego[condensed_lego['Theme']=='Marvel Super Heroes']['Price_per_piece']\n",
    "ninja = condensed_lego[condensed_lego['Theme']=='Ninjago']['Price_per_piece']\n",
    "technic = condensed_lego[condensed_lego['Theme']=='Technic']['Price_per_piece']\n",
    "print 'Star Wars: ',mannwhitneyu(population,sw), len(sw)\n",
    "print 'Marvel Super Heroes: ',mannwhitneyu(population,msh), len(msh)\n",
    "print 'Ninjago: ',mannwhitneyu(population,ninja), len(ninja)\n",
    "print 'Technic: ',mannwhitneyu(population,technic), len(technic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/mw_wo.png'>\n",
    "\n",
    "From the results above it can be seen that the p-value for the Star Wars theme is extremely low! In fact it is the only theme where the null hypothesis can be rejected. Marvel Super Heroes almost made it there with a p-value of 0.06.\n",
    "However when we look at the earlier histograms, it can be realised that this effect might be caused by the outliers that exists in the group. Given group size of over 300 one might be able to justify a further test to see what it would be like without such outliers. Therefore a second test is conducted with all the sets that have a price per piece above 1 USD removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "population = condensed_lego[condensed_lego['Price_per_piece']<1]['Price_per_piece']\n",
    "sw = condensed_lego[(condensed_lego['Theme']=='Star Wars')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "msh = condensed_lego[(condensed_lego['Theme']=='Marvel Super Heroes')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "ninja = condensed_lego[(condensed_lego['Theme']=='Ninjago')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "technic = condensed_lego[(condensed_lego['Theme']=='Technic')&(condensed_lego['Price_per_piece']<1)]['Price_per_piece']\n",
    "print 'Star Wars: ',mannwhitneyu(population,sw), len(sw)\n",
    "print 'Marvel Super Heroes: ',mannwhitneyu(population,msh), len(msh)\n",
    "print 'Ninjago: ',mannwhitneyu(population,ninja), len(ninja)\n",
    "print 'Technic: ',mannwhitneyu(population,technic), len(technic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/mw_woo.png'>\n",
    "\n",
    "By looking at the group size from the results it can be seen that 7 and 8 sets were excluded from Star Wars and Technic respectively. This drastically changes the outcome where the null hypothesis can not be rejected for any of the cases (Star Wars at around 0.1 has the lowest p-value amongst them still). The data removed accouts for less than 4% in each group so it can be said that for the majority of the group the pricing is not that different. It is when we include the complete record we would have some big outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSRP Regression Analysis<a class=\"anchor\" id=\"regression\"></a>\n",
    "\n",
    "The first aim of the project is to predict Lego sets MSRP in USD. The success criteria is to get within 15% of the true price. From the EDA we can see that the price varies approximately linearly with some of the variables such as number of pieces. Therefore it would be logical to start of with models such as Linear Regressions and its regualrised counterparts. However before fitting any models, it would be necessary to refine the data into a format that the modeling module can take as input. It would also be great if some further plotting demonstrating the relationships between features. The required moduels are loaded into the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "lego_model = lego.drop(['Item_Number','Name','Image_URL','USD_MSRP','query_id','adjust_coef'], axis=1)\n",
    "theme = pd.get_dummies(lego.Theme, drop_first=True, prefix='theme')\n",
    "subtheme = pd.get_dummies(lego.Subtheme, drop_first=True, prefix='subtheme')\n",
    "packaging = pd.get_dummies(lego.Packaging, drop_first=True, prefix='packaging')\n",
    "availability = pd.get_dummies(lego.Availability, drop_first=True, prefix='availability')\n",
    "theme_grp = pd.get_dummies(lego.theme_grp, drop_first=True, prefix='theme_grp')\n",
    "lego_model = lego_model.drop(['Theme','Subtheme','Packaging','Availability','theme_grp'],axis=1)\n",
    "lego_model.rating = lego_model.rating.astype(float)\n",
    "lego_model.review_num = lego_model.review_num.astype(float)\n",
    "lego_w_dummies = pd.concat([lego_model,theme,subtheme,packaging,availability,theme_grp],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lego_model_info.png'>\n",
    "\n",
    "This is the data frame without the dummies attached. Note that for the regression analysis the whole data set is used instead of the subgroup created in the hypothesis testing section. Dummies (also known as one hot encoded features are created for the categorical variables. This is because due to the nature of categorical variables being non-ordinal, it is not possible to just encode them into numbers. By one hot encoding them (i.e. having n columns for n categories) each row would have a value '1' where it is relevant and '0' where it's not. However it should be mentioned that this method can possibly create a huge feature space which could be challenging in terms of computational requirements and the introduction of overfitting.\n",
    "\n",
    "The following steps define the features and targets. In order to prevent overfitting, a train-test split will be carried out. This operation divides the data into a training set and a testing set. This allows us to fit the model with the train data and test it with the unseen test data to avoid training bias where models test itself against seen data and report potentially unrelistic good results . Typically the train data would include the majority of instances in the data set to allow good model fit. In this case, it is decided that a 4:1 train-test split to be used. A random state is specified so that the splitting methodology can be fixed and repeated tests can be carried out with the same splits to provide fairer comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = lego_w_dummies.drop('adj_USD_MSRP', axis=1)\n",
    "y = lego_w_dummies['adj_USD_MSRP']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "reg = LinearRegression()\n",
    "model = reg.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fitted the model and made a prediction, it is time to check the various metrics. There are three metrics that are commonly used in regression analysis namely mean squared error (MSE), mean absolute error (MAE) and R squared score (r2 score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'MSE: ',mean_squared_error(y_test,pred)\n",
    "print 'MAE: ',mean_absolute_error(y_test,pred)\n",
    "print 'R2: ', r2_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lr_rst.png'>\n",
    "\n",
    "In order to better visualise the model, the predicted price is plotted agains the actual price. In an ideal situation we sould observe a closely pcaked cluster which demonstrates a trend that represents that actual price equals to predicted price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.scatter(y_test, pred, color='midnightblue', alpha=.7)\n",
    "ax2.scatter(y_test, pred, color='midnightblue', alpha=.7)\n",
    "ax1.set(xlabel='True price',ylabel='Predicted price',title='Comparison between predicted price and actual price')\n",
    "ax2.set(xlabel='True price',ylabel='Predicted price',title='Comparison between predicted price and actual price',xlim=[0,600],ylim=[0,1200])\n",
    "ax2.plot(y_test,y_test,color='crimson',alpha=.7,label='True price line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lr_plt.png'>\n",
    "\n",
    "The plot on the left above is definetly not something that represents a good regression model. It can be observed that y axis has a scale of 1e8 which represents 100million. Therefore from the analysis we can see that a model is predicting a 50million price for one Lego set. This is not a reasonable price for any Lego set. There are other unreasonable outliers such as negative values too.\n",
    "\n",
    "However interestingly when we zoom in to limit true prices between 0-600 and predicted prices between 0-1200 (as shown in the right hand side plot), it can be seen that this regression is actually doing a fair job within this section. This can be shown with the 'True price line' which demonstrates what the plot would be like if the predictions were 100% accurate. We can see that in general the points in the graph lie relatively closely to the line.\n",
    "\n",
    "From the above information, it would be reasonable to assume that a linear model can perform well in this problem. However due to certain factors it can produce strange results for some cases. Overfitting would be a major contributor in this instability. \n",
    "\n",
    "In this model, there are 509 input features where all but 8 of them are categorical dummy variables. Intuitively it is reasonable to assume that not all the features are equally informative. On the other hand some features could be highly correlated. Either of those would affect the performance of the regression model.\n",
    "\n",
    "Therefore in order to try mitigating the effect of the described phenomenon, a Lasso regression and Ridge regression is performed as further investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "reg = Lasso()\n",
    "model = reg.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "score = cross_val_score(reg,X_train,y_train,cv=kf)\n",
    "maescore = cross_val_score(reg,X,y,cv=kf,scoring='neg_mean_absolute_error')\n",
    "msescore = cross_val_score(reg,X,y,cv=kf,scoring='neg_mean_squared_error')\n",
    "print 'MSE: ',mean_squared_error(y_test,pred)\n",
    "print 'MAE: ',mean_absolute_error(y_test,pred)\n",
    "print 'R2: ', r2_score(y_test,pred)\n",
    "print '10 fold CV average R2: ', np.mean(score)\n",
    "print '10 fold CV R2 standard deviation: ',np.std(score)\n",
    "print '10 fold CV average MAE: ', np.mean(maescore)\n",
    "print '10 fold CV MAE standard deviation: ', np.std(maescore)\n",
    "print '10 fold CV average MSE: ', np.mean(msescore)\n",
    "print '10 fold CV MSE standard deviation: ', np.std(msescore)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(y_test, pred, color='midnightblue', alpha=.7)\n",
    "ax1.set(xlabel='True price',\n",
    "        ylabel='Predicted price',\n",
    "        title='Comparison between predicted price and actual price',\n",
    "        xlim=[0,600],\n",
    "        ylim=[0,600])\n",
    "ax1.plot(y_test,y_test,color='crimson',alpha=.7,label='True price line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/lasso_rst.png'>\n",
    "<img src='assets/pics/lasso.png'>\n",
    "\n",
    "A similar model using Ridge regression was created as well. The results are as follows\n",
    "\n",
    "<img src='assets/pics/ridge_rst.png'>\n",
    "<img src='assets/pics/ridge.png'>\n",
    "\n",
    "The above plots and result figures have demonstrate clearly of how regularisations can effectively improve models performance. This is best demonstrated by the significant reduction in MSE and MAE. R2 score has increased from negative value to above 0.6 (Although the number of features has an affect on R2 scores, the comparison in this case is still valid because number of features are constant).  \n",
    "\n",
    "The principle of regularisation techniques is to add an extra term to the cost function which is related to the size of the coefficients. This would affect the process of minimising the cost function by limiting the magnitude of coefficents. \n",
    "\n",
    "The strength of coefficients can be altered by changing the 'alpha' parameter. Higher alpha represents stronger regularisation. It is possible to use Grid Search method in order to obtain the best 'alpha' value. \n",
    "\n",
    "#### Fine Tuning Lasso with Grid Search<a class=\"anchor\" id=\"lasso gscv\"></a>\n",
    "\n",
    "Lasso regression is the combination of linear regression and a 'l1' regularisation. This regularisation term is proportional to the absolute values of feature coefficients. A Grid Search method is used to try obtaining the best alpha value for such regularisation. A 10 fold cross validation is used in the Grid Search process to make sure the model is stable and be able to deal with unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'alpha':np.logspace(-2,2,5)}\n",
    "las = Lasso(max_iter=3000)\n",
    "clf = GridSearchCV(las,params,cv=kf)\n",
    "model = clf.fit(X_train,y_train)\n",
    "print 'Optimum alpha: ',model.best_params_\n",
    "print 'Best score: ',model.best_score_\n",
    "model.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/gscv_lasso_best_params.png'>\n",
    "<img src='assets/pics/gscv_lasso_grid_scr.png'>\n",
    "\n",
    "By looking at the results it can be said that the alpha value which yeidls the best result is 0.01. This means a small amount of regularisation is applied. We can also see that an alpha of 0.01 results in the least standard deviation of R2 scores in the model which suggests that it is also more stable. Using the best parameters, a new prediction for the test set is generated.\n",
    "\n",
    "<img src='assets/pics/opt_lasso_rst.png'>\n",
    "\n",
    "From the results above we can see that the performance of the test set have dropped despite using the optimised parameter. There could be many reasons that can cause such a result. Overfitting could be one of them. In this case the cross validation is carried out with the training set which might not have the exact same data structure as the testing set. Therefore when a model is highly optimised with the training set, the variance might be so high that it does not fit well with the testing set. This idea of overfitting is further supported by the alpha value of 0.01 which means little regularisation (in other words bias) is introduced. Gridsearch with the same parameters were carried out for Ridge regression and yielded the following parameters and test set results.\n",
    "\n",
    "<img src='assets/pics/ridge_best_params.png'>\n",
    "<img src='assets/pics/opt_ridge_rst.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient Analysis<a class=\"anchor\" id=\"Coefficient Analysis\"></a>\n",
    "\n",
    "Having performed regression analysis with various regularisation techniques, it would be useful to look at their resultant coefficient. This can provide an insight on what factors contributes most to the MSRP of Lego sets. It is important to note that both ends of the spectrums are to be examined because those are the features that drives the prices significantly (either up or down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_coef = zip(X_train.columns,ridge_model.coef_)\n",
    "ridge_coef = sorted(ridge_coef, key=lambda x:x[1], reverse=True)\n",
    "ridge_coef[:5]\n",
    "ridge_coef[:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge\n",
    "\n",
    "<img src='assets/pics/max_ridge_coef.png' style='float:left;'>\n",
    "<img src='assets/pics/min_ridge_coef.png'>\n",
    "\n",
    "Again a similar analysis was carried out for Lasso.\n",
    "\n",
    "<img src='assets/pics/lasso_max_coef.png' style='float:left;'>\n",
    "<img src='assets/pics/lasso_min_coef.png'>\n",
    "\n",
    "Lasso Regression ranks the coefficients similarly to Ridge. 80% of their top 5 are the same. However it can be seen that Lasso ranks 'Serious Play' even more importantly at 430. Lasso also reviews a confusing variable naming strategy used in the anlysis. It can be seen that Mindstorms appeared in both the highest and lowest coefficient groups. This can be due to the fact that the term 'Mindstorms' appeared in different columns (such as Theme, Subtheme or theme_grp). Therefore it is difficult to analyse what is the actual factor that drives the prices. An improvement can be made by adding prefix for the dummy features.\n",
    "\n",
    "If we look closer at the coefficients for Lasso Regression. We can find that a lot of the coefficients are set to 0. This is a special property of Lasso due to the fact that Lasso's regularised parameter is proportional to the coefficient (but not its squared value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA <a class=\"anchor\" id=\"PCA\"></a>\n",
    "\n",
    "Principal Components Analysis (PCA) is a technique used to re-orient how we (or a model) looks at the data. It is done by determining a line where the data demonstrates highest variance (in mathematical terms highest Eigen values) and project the data points onto such line (which is the Eigen vector). This is done repeatedly in the order of variance explained. \n",
    "\n",
    "It is commonly found out that it was beneficial to transform raw data in Principal Components (PCs) before fitting a model. This is because by selecting the PCs that explains most of the variance in the data, we would lower the risk of overfitting the model. It would also reduce the size of feature spaces which speeds up computing processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(X)\n",
    "pca_X = pca.transform(X)\n",
    "pca_X_train, pca_X_test, pca_y_train, pca_y_test = train_test_split(pca_X,y,test_size=0.2,random_state=42) \n",
    "print len(np.cumsum(pca.explained_variance_ratio_))\n",
    "print np.cumsum(pca.explained_variance_ratio_)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pca_stat.png'>\n",
    "\n",
    "By performing a Principal Component Analysis (PCA), it is found out that the first 3 Principal Components (PCs) can explain more than 99.99% of the variance. This is rather interesting since our input feature space consists of 508 features. Further regression analysis would be performed to see how this transformation would affect the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "px_train = pca_X_train[:,:3]\n",
    "px_test = pca_X_test[:,:3]\n",
    "rid = Ridge(max_iter=3000, alpha=1)\n",
    "rid_score = cross_val_score(rid,px_train,pca_y_train,cv=10)\n",
    "pca_rid_model = rid.fit(px_train,pca_y_train)\n",
    "pca_rid_pred = pca_rid_model.predict(px_test)\n",
    "print 'CV scores on train set: ',rid_score\n",
    "print 'Mean CV score on train set: ',np.mean(rid_score)\n",
    "print 'MSE: ',mean_squared_error(pca_y_test,pca_rid_pred)\n",
    "print 'MAE: ',mean_absolute_error(pca_y_test,pca_rid_pred)\n",
    "print 'R2: ', r2_score(pca_y_test,pca_rid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pca_rst.png'>\n",
    "\n",
    "Although 99% of the variance is explained, the performance of the test set has not improved from pre-PCA model. MSE is lowered from 938 but MAE has increased from 12. R2 remains quite similar with values of 0.59 and 0.60. Therefore it is decided that cross validated models with various numbers of PCs as features should be created and tested to gain a better understanding of how number of PCs affects regression performance in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "las_cv_r2= []\n",
    "las_cv_mae = []\n",
    "las_cv_mse = []\n",
    "for i in range(1,508):\n",
    "    pred = cross_val_predict(las,pca_X_train[:,:i],pca_y_train,cv=8)\n",
    "    las_cv_r2.append(r2_score(pca_y_train,pred))\n",
    "    las_cv_mae.append(mean_absolute_error(pca_y_train,pred))\n",
    "    las_cv_mse.append(mean_squared_error(pca_y_train,pred))\n",
    "fig = plt.figure(figsize=(18,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax1.plot(range(1,508), las_cv_r2, color='midnightblue', alpha=.7)\n",
    "ax2.plot(range(1,508), las_cv_mae, color='midnightblue', alpha=.7)\n",
    "ax3.plot(range(1,508), las_cv_mse, color='midnightblue', alpha=.7)\n",
    "ax1.set(xlabel='Number of PCs',\n",
    "        ylabel='10 fold CV R2',\n",
    "        title='PCs used and R2 score in Lasso after PCA',\n",
    "        xlim=[0,550])\n",
    "ax2.set(xlabel='Number of PCs',\n",
    "        ylabel='10 fold CV MAE',\n",
    "        title='PCs used and MAE in Lasso after PCA',\n",
    "        xlim=[0,550])\n",
    "ax3.set(xlabel='Number of PCs',\n",
    "        ylabel='10 fold CV MSE',\n",
    "        title='PCs used and MSE in Lasso after PCA',\n",
    "        xlim=[0,550])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pc_num.png'>\n",
    "\n",
    "The above plots clearly shows that beyond 100 PCs, the improvement in regression metrics with the number of PCs are very minimal. It is interesting to find that although 99% of the variacne is explained by the first 3 PCs, the MSE and MAE continue to drop as more PCs are included in the analysis. It can be seen that MAE never really stop decreasing until we hit a point of around 360 PCs. MSE has a more stable curve and hits the minimum at around 380 PCs. However we should always question how confident we are in the gain in performance especially when the gain is very small. \n",
    "\n",
    "After gaining the above insight, it is decided to fit a model and perform a prediction on the test set using the first 100 PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "px_train = pca_X_train[:,:100]\n",
    "px_test = pca_X_test[:,:100]\n",
    "las = Lasso(max_iter=3000, alpha=0.01)\n",
    "las_score = cross_val_score(las,px_train,pca_y_train,cv=10)\n",
    "pca_las_model = las.fit(px_train,pca_y_train)\n",
    "pca_las_pred = pca_las_model.predict(px_test)\n",
    "print 'CV scores on train set: ',las_score\n",
    "print 'Mean CV score on train set: ',np.mean(las_score)\n",
    "print 'MSE: ',mean_squared_error(pca_y_test,pca_las_pred)\n",
    "print 'MAE: ',mean_absolute_error(pca_y_test,pca_las_pred)\n",
    "print 'R2: ', r2_score(pca_y_test,pca_las_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/pca_reg_rst.png'>\n",
    "\n",
    "As we can see from the results the MSE has drastically reduced from 938 to 598. This represents a reduction of 340 which is  a 36% improvement. MAE has reduced by 0.5. R2 score has increased (despite the number of features has decreased) to 0.73 from 0.59 which is a significant 24% improvement. \n",
    "\n",
    "These results have reinforced the findings earlier that by further increasing the number of PCs up to a certain point, it is still able to improve model performance despite the fact that gain in variance explained is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(pca_y_test,pca_las_pred, color='midnightblue', alpha=.6,label='100PCs Lasso')\n",
    "ax1.scatter(y_test, lasso_pred, color='darkorange', alpha=.6, label='Normal Lasso')\n",
    "ax1.set(xlabel='True price',\n",
    "        ylabel='Predicted price',\n",
    "        title='Comparison between predicted price and actual price with 100 PCs using Lasso',\n",
    "        xlim=[0,600],\n",
    "        ylim=[0,600])\n",
    "ax1.plot(y_test,y_test,color='crimson',alpha=.7,label='True price line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'assets/pics/pca_compare.png'>\n",
    "\n",
    "When we try to compare the vanilla Lasso and 100PCs Lasso it is not very obvious where the improvements are made. We can see that the blue points (100PCs Lasso) might be slightly closer to the 'True price line'. However if we look more carefully we can see that one points that was previously seriously mis-predicted (point at around (45,510)) is now in a much better position. This in fact reflects what we have discovered in the results comparison earlier on. The great reduction in MSE is due to the fact that MSE is a metric that is heavily influenced by outlier. If an outlier is removed the MSE can vary by a large amount. On the ohter hand MAE is less prone to such problem and hence we can only observe a slight improvement in MAE. \n",
    "\n",
    "The R2 score observed in this model can be considered reasonable. The interpretation of R2 score is the proportion of variance explained by the model to the total variance. It can be seen that our model is able to explain more than 70% of the variance. This can be considered as reasonable score since pricing of a certain Lego set is a result from various activites where human decisions are invloved. Threfore it is not as rigid as some other problems such as testing for physical phenomenon in the scientific research (which can yield a R2 score of over 90%). \n",
    "\n",
    "#### Regression Conclusion<a class=\"anchor\" id=\"Conclusion R\"></a>\n",
    "Initial regression analysis was carried out to try predicting the MSRP from other features. The dataset is divided into a training set and testing set in order to test the model with unseen data. Cross validation was also applied through out the investigation to further ensure that the chance of over fitting is reduced to a minmum. In the first attempt a linear regression model is fitted directly with the feature data. The testing result produced in the test set is very unreasonable with unrealistic MSRP such as below 0 and in the millions. it is suggested that the model could be seriously overfitting and therefore Lasso and Ridge were performed. These models produce significantly better results with MSE and MAE reduced to around 800-900 and 12.5-14 respectively. \n",
    "\n",
    "Further investigation with PCA was carried out. It is found out that only 3PCs (out of the total 508) can explain more than 99% of the variance. However it is found out that the model performance max out when aroudn 100 PCs are used. PCA improves the overall performance of the Lasso model slightly with a 0.5 reduction of MAE. However it has successfully removed an outlier which significantly improves MSE.\n",
    "\n",
    "In conclusion it is possible to achieve a reasonable result with regression analysis. However the aim of hitting the 15% mark appears to be a target too far. With the current MAE the Lego set MSRP has to be 84 USD for this situation to be a success. From the EDA we know that this is definetly not the case so unfortunately this can not be considered as a success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desirability Analysis (Classification)<a class=\"anchor\" id=\"clf\"></a>\n",
    "\n",
    "Apart from trying to predict MSRP for Lego sets, the other aim of the project is to successfully classify Lego sets into categories of being nomal sets or desirable sets. The success criteria is to perform better than the baseline accuracy which is yet to be determined. From the EDA we have concluded that the 'wanted' column will be used as our metric. A threshold value would be set and depending on the counts for each instance in the wanted column, they will be classified as being either desirable or normal. It would be reasonable to start off with such categorsation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, classification_report, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column which serve as the classification labels is to be created. At this point we would have to define a threshold line to determine which sets are desirable. This threshold can be set at the 80 percentile hence dividing the data into 2 groups with the top 20% being desriable. This sets our baseline accuracy to 80%. This is becuase if we blindly guess that all instances have a label of 0, we will get an 80% accuracy. Therefore a good model should perform better than this accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt = np.percentile(lego.wanted, 80)\n",
    "lego_w_dummies['labels'] = lego_w_dummies.wanted.apply(lambda x:0 if x<pt else 1)\n",
    "X = lego_w_dummies.drop(['wanted','labels'], axis=1)\n",
    "y = lego_w_dummies['labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the creation of features and labels matrix, a train-test split is be carried out in order to prevent overfitting. Again this operation divides the data into a training set and a testing set. This allows us to fit the model with the train data and test it with the unseen test data. Again the train data would include the majority of instances in the data set to allow good model fit. Therefore it is decided that a 4:1 train-test split to be used. Since the features and target are different from the regression analysis, the train test split has to be redefined.\n",
    "\n",
    "The first model to try is Logistic Regression. Similar to Linear Regression used in predicting MSRP, Logistic Regression is also a linear model. It relates the log of odds being positive (label = 1) to the predicting features. The assumption of the the model is that the log of odds varies linearly with the predicting features. The advantage of this model is that it is easy to understand. Also being a linear model it is relatively simple and quick to fit hence is suitable to be used as the first model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "model = clf.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "print 'Accuracy: ',accuracy_score(y_test,pred)\n",
    "print 'ROCAUC: ',roc_auc_score(y_test,pred)\n",
    "print classification_report(y_test,pred)\n",
    "print confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/logreg_rst.png'>\n",
    "\n",
    "The results above shows that Logistic Regression has a very respectable performance of 93% accuracy. This is complemented with a good ROC score of 0.88. ROC score describes the area under the ROC curve. In the Scikit Learn implementation, the baseline ROC score would be 0.5. That means if you are randomly guessing, the mean ROC score that you would likely to get is very close to 0.5.\n",
    "\n",
    "Looking at the classification report, it seems that the model is better at classifying labels with 0s than labels with 1s. This is expected because we have much more instances that have a true label of 0. However it should be noted that this train-test split is not stratified. Therefore a model performing well in this case might not perform well in other cases where the data is splitted differently. In order to varify that we have got a stable model, a 10 fold cross validation is carried out with the training data to let us gain more insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sk = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "score = cross_val_score(clf,X_train,y_train,cv=sk)\n",
    "print 'Average accuracy: ',np.mean(score)\n",
    "print 'Accuracy standard deviation: ', np.std(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/logreg_cv.png'>\n",
    "\n",
    "The average accuracy is 94.5% with a minimum accuracy of 90.9% and maximum accuracy of 97.1%. The standard deviation is 0.010 which can be translated to an understanding that 95% of the accuracy is likely to fall between 94.5% +- 2%. This is sufficient to prove that our model is relatively stable and should be able to handle unseen data with reasonable performance. However it is still possible to try improving this model.\n",
    "\n",
    "The first step is to use Grid Search in order to find out the optimum hyper-parameter for Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'penalty':['l1','l2'], 'C':np.logspace(-2,2,5)}\n",
    "model = GridSearchCV(clf,params,cv=sk).fit(X_train,y_train)\n",
    "model_logreg = model.best_estimator_.fit(X_train,y_train)\n",
    "pred = model_logreg.predict(X_test)\n",
    "print 'Accuracy: ',accuracy_score(y_test,pred)\n",
    "print 'ROCAUC: ',roc_auc_score(y_test,pred)\n",
    "print classification_report(y_test,pred)\n",
    "print confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/gscv_logreg.png'>\n",
    "<img src='assets/pics/gscv_logreg_test.png'>\n",
    "\n",
    "The above analysis demonstrated that by using Grid Search we sucessfully find a better parameter to fit the model. The penalty is set to be 'l1'. This has a very similar effect comparing with Lasso when regression analysis is performed. L1 regularisation has a penalty term related to the absolute values of the coefficients. Like Lasso it is also possible to set coefficients of certain features to 0 which can be considered as automatic feature selection. In Scikit Learn implementaion the penalty term is denoted by a prameter named 'C'. It is inversely proportional to the regularisation coefficient alpha. This means that as 'C' gets larger, alpha gets smaller i.e. less regularisation is applied. Grid Search returned a 'C' value of 10 which is higher than default. This means that for this problem a lower regularisation seems to be beneficial.\n",
    "\n",
    "The following section demonstates the use of other classifiers and their default parameters as comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "DecisionTree = DecisionTreeClassifier()\n",
    "RandomForest = RandomForestClassifier()\n",
    "AdaBoost = AdaBoostClassifier()\n",
    "ExtraTrees = ExtraTreesClassifier()\n",
    "GradientBoost = GradientBoostingClassifier()\n",
    "SVM = SVC()\n",
    "\n",
    "\n",
    "clf_lst = ['KNN','DecisionTree','RandomForest','AdaBoost','ExtraTrees','GradientBoost','SVM']\n",
    "clf_df = pd.DataFrame(np.zeros([7,4]), index=clf_lst, columns=['Test_accuracy','Test_ROC','CV_accuracy','CV_std'])\n",
    "\n",
    "\n",
    "def create_prediction(clf,X_train,X_test,y_train,y_test):\n",
    "    model = clf.fit(X_train,y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    score = cross_val_score(clf,X_train,y_train,cv=sk)\n",
    "    a_s = accuracy_score(y_test,pred)\n",
    "    roc = roc_auc_score(y_test,pred)\n",
    "    score_mean = np.mean(score)\n",
    "    score_std = np.std(score)\n",
    "    return [a_s, roc, score_mean, score_std]\n",
    "\n",
    "for clf in clf_lst:\n",
    "    c = eval(clf)\n",
    "    clf_df.loc[clf] = create_prediction(c,X_train,X_test,y_train,y_test)\n",
    "\n",
    "clf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/clf_rst_df.png'>\n",
    "\n",
    "The brief analysis above shows that ensemble methods do have a significant edge in this calssification problem. By look at the performance of the ensebles, it is possible to say that they are on par with the tuned Logistic Regression. Therefore it is decided that Grid Search will be used to further enhance those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators':[100,300,500], \n",
    "          'min_samples_split':[2,4,6],\n",
    "          'max_depth':[3,5,7]}\n",
    "model = GridSearchCV(GradientBoostingClassifier(),params,cv=sk, n_jobs=-1).fit(X_train,y_train)\n",
    "print 'Best parameters: ',model.best_params_\n",
    "print 'Best accuracy: ',model.best_score_\n",
    "optimised_clf = model.best_estimator_.fit(X_train, y_train)\n",
    "pred = optimised_rf.predict(X_test)\n",
    "print 'Accuracy: ',accuracy_score(y_test,pred)\n",
    "print 'ROCAUC: ',roc_auc_score(y_test,pred)\n",
    "print classification_report(y_test,pred)\n",
    "print confusion_matrix(y_test,pred)\n",
    "\n",
    "from ricky_custom import plot_confusion_matrix, plot_roc\n",
    "plot_confusion_matrix(confusion_matrix(y_test,pred), classes=optimised_clf.classes_)\n",
    "plot_roc(y_test,optimised_clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/gscv_gb.png'>\n",
    "<img src='assets/pics/opt_gb.png'>\n",
    "<img src='assets/pics/gb_cm.png'>\n",
    "<img src='assets/pics/gb_roc.png'>\n",
    "\n",
    "From the confusion matrix and ROC curve plot above we can see that the Gradient Boosting model has a very respectable performance. Both the test score and cross validated score are over 95%. The high ROC and accuracy scores means the prediction gets most of the cases right. By further analysing the confusion matrix we can see that there are more false negatives than false positives. This is demosntrated in the recall score for class 1 in the classification report.\n",
    "\n",
    "Apart from optimising the good models, it is also interesting to try improving the less capable models. For this problem SVM and kNN are not performing very well comparing to the others. They have a test set score of 80% and 88% respectively. However we know that both algorithm rely heavily on the distances between data points in a feature space. Therefore feature scaling will be applied in other to try obtaining better results for these 2 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling for the Distance-Sensitive Models<a class=\"anchor\" id=\"feature scaling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "std_lego = pd.DataFrame(scaler.fit_transform(lego),columns=lego.columns)\n",
    "std_lego_w_dummies = pd.concat([std_lego,theme,subtheme,packaging,availability,theme_grp],axis=1)\n",
    "std_X = std_lego_w_dummies.drop(['wanted','labels'], axis=1)\n",
    "std_X_train, std_X_test, y_train, y_test = train_test_split(std_X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_clf_df = pd.DataFrame(np.zeros([7,4]), index=['KNN','SVM'], columns=['Test_accuracy','Test_ROC','CV_accuracy','CV_std'])\n",
    "for clf in ['KNN','SVM']:\n",
    "    c = eval(clf)\n",
    "    std_clf_df.loc[clf] = create_prediction(c,std_X_train,std_X_test,y_train,y_test)\n",
    "\n",
    "std_clf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/std_clf.png'>\n",
    "\n",
    "After applying the StandardScaler to transform the feature, it is evident that both kNN and SVM have received a large improvement in terms of prediction accuracy and ROC score. The performance of the kNN classifier is now even comparable to the tuned Logistic Regression model. SVM whilst not as good as the others, has improved its accuracy by 10%. This is a significant improvement as it is now performing better than the baseline accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance  <a class=\"anchor\" id=\"feature importance\"></a>\n",
    "\n",
    "Similar to the coefficient analysis carried out in regressions, feature importance analysis can be carried out for tree based models. Tree based models make their splits according to entropy or gini which both are related to infomation gain. Therefore graph can be plotted to gain a deeper understand of what the tree is actually relying on to make its decisions. The following sample would show how a data scientist can plot a tree. Although a maximum depth of 3 might not create the most accurate tree, it would allow a better visualisation to be produced for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "clf = dt\n",
    "model = clf.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "from os import system\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def build_tree_image(model, filename='tree.png'):\n",
    "    dotfile = open(\"tree.dot\", 'w')\n",
    "    export_graphviz(model, out_file = dotfile, feature_names = X.columns,filled=True,\n",
    "                    rounded=True,special_characters=True)\n",
    "    dotfile.close()\n",
    "    system(\"dot -Tpng tree.dot -o {0}\".format(filename))  \n",
    "build_tree_image(model,'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/tree.png'>\n",
    "\n",
    "From the diagram above we can see that ownership is a very important factor as it is being called multiple times. Other factors used include adjusted MSRP, number of pieces and whether the set has a Star Wars theme.\n",
    "\n",
    "#### Classification Conclusion <a class=\"anchor\" id=\"Conclusion C\"></a>\n",
    "The second aim of the project is to successfully classify which Lego sets are desirable. Refering back to the EDA it is decided that as initial analysis, the definition of how desirable a Lego set is by its number of 'wanted' votes. It is set that the top 20% of the data is considered desirable while others are considered normal. This 20% threshold is set casually without much supporting reason. However we are in the initial model fitting stage and this would suffice.  A Logistic Regression model is tried initially. This is because being a linear model Logistic Regressin is relatively simple and easy to understand. It is also not computationally intensive which should give us a good understanding of the situation. The inital result returned a promising 93% for the testing set and 94% in cross validation of the training set. This is surprisingly high and this could be due to the fact that regularisation is applied as standard. Through Grid Search a better parameter is obtained which further improves accuracy by 1%. \n",
    "\n",
    "Due to the promising result it is decided that other models can be fitted and see how they perform. It is found out that the ensemble models (e.g. Random Forest and Gradient Boost) perform very well out of the box which can rival tuned Logistic Regression. With some parameters tuning, a test accuracy of 95% can be obtained by Gradient Boosting. \n",
    "\n",
    "However some models do not perform well in standard form namely SVM and kNN. The common feature of these models is that the distance between data points drastically affects classification performance. Therefore feature scaling was applied which significantly improves their results (88% to 94% and 80% to 90%) respectively.\n",
    "\n",
    "Therefore in conclusion it is possible to say that one can confidently indentify desirable Lego sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beating the Market\n",
    "\n",
    "Every Lego sets have a Manufacturer Suggested Retail Price which, obviously, is a guideline provided by Lego to help individual retailers in pricing their products. However due to different real world situations the true retail price of these Lego sets can deviate from the MSRP. For example if a Lego set is discontinued, its price might tend to increase. On the other hand if new versions of the same set are released, it might bring down the price of an older set. Therefore in this investigation, we are going to compare the current price with their respective MSRP and see if there are certain factors that affects these price changes. According to this article written by the <a href='http://www.telegraph.co.uk/investing/shares/lego-a-better-investment-than-shares-and-gold/'>Telegraph</a>, it is suggested that some Lego sets can even be considered as better investments comparing to traditional investment products. Hence it would be interesting to identify these market beaters and explore what are some of the properties that causes them to perform so well.\n",
    "\n",
    "#### Web Scraping<a class=\"anchor\" id=\"Web Scraping\"></a>\n",
    "\n",
    "In order to find out which Lego sets have beaten the market, it is necessary to obtain updated price figures for individual Lego sets. This information can not be obtained through the Brickset API and the API from Bricklink is difficult to use as it requires static IP address which is an inconvenience in my case as I have to constanly travel between home and school. Therefore a web scrapping solution was employed. The code for the scrapping is shown below and the explanation would follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pickle\n",
    "from boto.s3.connection import S3Connection, Bucket, Key\n",
    "import urllib2\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "def get_box_sales(lst):\n",
    "    box_sales = {}\n",
    "    for i in lst:\n",
    "        print i\n",
    "        data = get_set_sales_info(i)\n",
    "        box_sales[i] = data       \n",
    "        if len(box_sales)%5 == 0:\n",
    "            print 'Collected ',len(box_sales),' items. ',len(lst)-len(box_sales), ' remains.'\n",
    "    return box_sales\n",
    "\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'w') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def change_format(lst):\n",
    "    new_lst = np.array([float(i.replace(',','')) for i in lst])\n",
    "    return new_lst\n",
    "\n",
    "def get_set_sales_info(query_id):\n",
    "    wd = webdriver.PhantomJS()\n",
    "    wd.get('http://www.bricklink.com/v2/catalog/catalogitem.page?S=%s#T=P' %(query_id))\n",
    "    WebDriverWait(wd, timeout=20).until(lambda x: x.find_element_by_tag_name('tr'))\n",
    "    page_source = wd.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    tables = soup.findAll(\"td\",valign=\"top\")\n",
    "    \n",
    "    try:\n",
    "        price_finds = re.findall(r'\\$(.+?\\.\\d+?)</td',str(tables[2]))\n",
    "        qty_finds = re.findall(r'>(\\d+)</td',str(tables[2]))\n",
    "        cur = 'USD'\n",
    "        if price_finds == []:\n",
    "            print 'try GBP'\n",
    "            price_finds = re.findall(r'GBP (.+?\\.\\d+?)</td',str(tables[2]))\n",
    "            qty_finds = re.findall(r'>(\\d+)</td',str(tables[2]))\n",
    "            cur = 'GBP'\n",
    "            \n",
    "    except:\n",
    "        print 'no data'\n",
    "\n",
    "    try:\n",
    "        price = change_format(price_finds)\n",
    "        qty = change_format(qty_finds)\n",
    "        mean = np.sum(price*qty)/np.sum(qty)\n",
    "        stdev = np.sqrt(np.sum(qty*((price-mean)**2))/qty-1)\n",
    "        print mean, stdev, cur\n",
    "        return (mean,stdev,sum(qty),cur)\n",
    "    \n",
    "    except:\n",
    "        print 'no record found'\n",
    "        pass\n",
    "    \n",
    "    print tables                    \n",
    "\n",
    "df_box = pd.read_csv('box.csv')\n",
    "query_lst = df_box.query_id.tolist()\n",
    "boxes = get_box_sales(query_lst)\n",
    "save_obj(boxes,'box_qty_price') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/Bricklink_example.png'>\n",
    "\n",
    "The above screen is captured to demonstrate the format of page that the scripts scrapes from. The Selenium module is used instead of a simple urllib/requests module. This is because the data in the tab is generated (or retrieved) by the JavaScript in the page which takes a certain time to load. Normal get methods can not compensate for this time lag and would result in a data not found. Selenium being a module used for web testing allows users to extract contents with delays or 'search until found' function which proves to be effective in this situation. However the down side of this script (overall) is that the scraping process is relatively low. In order to reduce the risk of broken connection during scraping, the task was carried out in an screened Amazon Web Services (AWS) EC2 instance. However due to the server location/browser cookies settings, the currency displayed by Bricklink in the EC2 instance is not stable which switches between USD and GBP. Hence a \"try/except\" and \"if\" statements were incorprated to capture the maximum amount of information available. \n",
    "\n",
    "The script is designed to get the current price of new sets. If there are no sets on sale currently, then that particular set would not be included in the analysis where current price is relevant. The price and quantity for each entries were recorded and the mean and sample standard deviation (using the Bessel's Correction) were calculated. The sample sizes and currency was also concluded should exchanging be applied.\n",
    "\n",
    "The results are stored in a dictionary with item query number (item number - revision) as the key. The completed dictionary is then packaged into a pickle item which can be transferred to the local disk or S3 storage space as needed.\n",
    "\n",
    "It is decided that this investigation would be limited to those sets that have boxes as the packaging method. It is because box sets are more commonly seen in retailers. Also from the EDA we know that there are some obvious outliers in the data (which are usually individual packs/accessories). By limiting the investigation to boxes only we would be aiming to provide the greatest value by indicating good value sets that one can easily purchase. A datagrame consists of only boxed sets were prepared earlier and it will be use through out this part of the project. A pickled dictionary called 'merged_box' was also used. It essentially provides the different prices for boxed sets like the above script. However it doesn't have information about sample sizes and standard deviation and that was why the above script would still need to be used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_dic = pickle.load(open('merged_box.pkl','r'))\n",
    "box = pd.read_csv('box.csv')\n",
    "def merge_current_data(query_id,data,status):\n",
    "    if query_id in price_dic:\n",
    "        cu = price_dic[query_id].loc[data,'current_'+status]\n",
    "        if cu==0:\n",
    "            cu = price_dic[query_id].loc[data,'average_'+status]\n",
    "        return cu\n",
    "    return np.nan\n",
    "\n",
    "# Scrapped data is in GBP so it is necessary to convert back into USD for comparison\n",
    "gbp_usd = 1.24\n",
    "box['current_new_price'] = box.query_id.apply(lambda x:merge_current_data(x,'Qty Avg Price','new'))*gbp_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 81 NaN items in the data frame. This is because the scrapping function can only take information for sets that defined as 'set' from Bricklink. Some of the sets, although they are packaged in a box, fell within other categories. Therefore we are not able to obtain information about them. However as analysis purpose, these sets can be dropped and one should still be able to fit and analyse models with the remaining data. Then the price difference between MSRP and current price was calcualted. A categorical feature named 'appreciate' was created to differentiate those sets that had appreciated from those had not. Appreciation has a very simple definition here being current price is bigger than MSRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box['price_difference'] = box['current_new_price'] - box['adj_USD_MSRP']\n",
    "box['difference_per_cent'] = box['price_difference']/box['adj_USD_MSRP']\n",
    "box['appreciate'] = box['price_difference'].apply(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this definition makes classification tasks very challenging. By doing a simple value counts it is discovered that the baseline accuracy was 94%! This implied the data would be hugely imbalanced. Although this difficulty can possibly be tackled by methods such as resampling or shifting the weight of classifiers, it would be beneficial to re-think what it is that we are trying to predict and the logic behind. \n",
    "\n",
    "#### Confirm Appreciation<a class=\"anchor\" id=\"Confirm Appreciation\"></a>\n",
    "\n",
    "It is known that the 'current price' for Lego sets were obtained from an average of listed sales. One might wonder what are the ranges of those listed prices and how do we know that the 'appreciation' is real rather than noise? In order to solve this problem, hypothesis tests have to be carried out. Since the sample data had a range of values and it is desired to check whether those are different from the hypothesised value (MSRP), one sample t-tests would be carried out. As mentioned before in order to justify the usage of t-test one must ensure the samples are normally distributed. There are graphical or numerical methods to check for nomality. The down side of that would be massively complicate the whole analysis. Therefore normality was only assumed but not confirmed - which seems reasonable as this is usually the case in real life. The code used to carry out the tests is showed below. Note that the pickle file loaded here was the result from the scrapping script demonstrated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def get_std(x):\n",
    "    if box_qty_price[x] != None:\n",
    "        return box_qty_price[x][1]\n",
    "    else:\n",
    "        return np.nan\n",
    "def get_sample_size(i):\n",
    "    if box_qty_price[i] != None:\n",
    "        return box_qty_price[i][2]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def get_t (row):\n",
    "    t = (row['current_new_price']-row['adj_USD_MSRP'])/(row['new_price_std']/np.sqrt(row['new_price_sample_size']-1))\n",
    "    return t\n",
    "\n",
    "def get_p(row):\n",
    "    p = stats.t.sf(row['t_stat'], row['new_price_sample_size']-1)\n",
    "    return p\n",
    "box_qty_price = pickle.load(open('box_qty_price.pkl','r'))\n",
    "box['new_price_std'] = box['query_id'].apply(get_std)\n",
    "box['new_price_sample_size'] = box['query_id'].apply(get_sample_size)\n",
    "box_cleaned = box.dropna()\n",
    "box_cleaned = box_cleaned[(box_cleaned['adj_USD_MSRP']!=0)&(box_cleaned['new_price_std']!=0)]\n",
    "box_cleaned['t_stat'] = box_cleaned.apply(get_t, axis=1)\n",
    "box_cleaned['p_value'] = box_cleaned.apply(get_p, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 95% confidence interval is used hence a p-value of 0.05 or less is required to reject the nul hypothesis. It turns out that this analysis reduces the appreciated sets ration from 94% to 89%. The appreciation rate has to be worked out in order to compare with the market rate. The annual return serves as a convenient and easy to understand metric for comparison so it wouold be used. Since it is still early in 2017, the age of the set would be worked out by subtracting the release year from 2016. Then the 'difference in per cent would be divided by the age. It has to be noted that this calculation had not taken compunding effect into consideration. However with numbers that are small and relatively short time period, it should serve as a fair estimation. FTSE is the market that was compared to. It is because it is the best known market (index) in the UK and hence most relevant and accessible. The annual return was obtained from the <a href='http://www.ftse.com/Analytics/FactSheets/Home/DownloadSingleIssue?issueName=UKX'>FTSE report</a> which worked out to be 19.1%. Hence if the appreciation rate of a Lego set is larger than the FTSE return and is proved to be significant, it would be registerd as '1' under the beat_ftse column. A value counts operations idicates that the baseline accuracy for this classification would be around 52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box_cleaned['age'] = 2016-box_cleaned['Year'].astype(float)\n",
    "box_cleaned['ap_rate'] = box_cleaned['difference_per_cent']/box_cleaned['age']\n",
    "ftse_return = 0.191\n",
    "box_cleaned['beat_ftse'] = box_cleaned.apply(lambda x:1 if ((x.loc['ap_rate']>ftse_return)&(x.loc['appreciate_significant']==1)) else 0, axis=1)\n",
    "\n",
    "# Dropping all the columns with unique features which do not add information to the models\n",
    "box_cc = box_cleaned.drop(['Item_Number','Name','Image_URL','USD_MSRP','query_id','adjust_coef',\n",
    "                           'current_new_price','price_difference','difference_per_cent','appreciate',\n",
    "                           't_stat','p_value','ap_rate','new_price_std','new_price_sample_size',\n",
    "                           'appreciate_significant','wanted','owned','rating','review_num','growth_rate'], axis=1)\n",
    "\n",
    "# Get dummies for categorical variables\n",
    "theme = pd.get_dummies(box_cc.Theme, drop_first=True)\n",
    "subtheme = pd.get_dummies(box_cc.Subtheme, drop_first=True)\n",
    "packaging = pd.get_dummies(box_cc.Packaging, drop_first=True)\n",
    "availability = pd.get_dummies(box_cc.Availability, drop_first=True)\n",
    "theme_grp = pd.get_dummies(box_cc.theme_grp, drop_first=True)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "box_cc = box_cc.drop(['Theme','Subtheme','Packaging','Availability','theme_grp'],axis=1)\n",
    "\n",
    "box_wd = pd.concat([box_cc,theme,subtheme,packaging,availability,theme_grp],axis=1)\n",
    "no_box_wd = pd.concat([theme,subtheme,packaging,availability,theme_grp],axis=1)\n",
    "X = box_wd.drop('beat_ftse',axis=1)\n",
    "y = box_wd['beat_ftse']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "clf_log = LogisticRegression()\n",
    "model_log = clf_log.fit(X_train,y_train)\n",
    "pred_log = model_log.predict(X_test)\n",
    "score_log = cross_val_score(clf_log,X,y,cv=sk)\n",
    "print accuracy_score(y_test,pred_log)\n",
    "print classification_report(y_test,pred_log)\n",
    "print confusion_matrix(y_test,pred_log)\n",
    "print score_log\n",
    "print score_log.mean()\n",
    "print score_log.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/ftse_log_reg.png'>\n",
    "\n",
    "Using a simple default Logistic Regression yields an accuracy of 73% with the test set and cross validated score of 75%. This clearly beats the baseline accuracy! With this promising result, the regression coefficients are examined. \n",
    "\n",
    "<img src='assets/pics/max_logreg_coef.png' style='float:left;'>\n",
    "<img src='assets/pics/min_logreg_coef.png'>\n",
    "\n",
    "It appears that being in an airport theme and Legoland exclusive are strong indicators that a set would beat the market. It is interesting to see Licensed showed up as well because it is a group type describing that it is a licensed product (e.g. Marvel Super Heroes). On the other hand we have some themes that are ... well...less popular hence it is not a surprised that sets in those theme had a harder time in trying to beat the market. \n",
    "\n",
    "#### XGBoost and Voting<a class=\"anchor\" id=\"xgb\"></a>\n",
    "\n",
    "Then an ensemble method is used to perform this classification task. It is called Xgboost and it is one of the most popular algorithm amongst the data scinece community right now. This is because it is a well optimised algorithm which provides good performance in terms of both speed and accuracy. Kaggle cometititon or oftenly won by this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score,classification_report,confusion_matrix\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "score = cross_val_score(xgb,X,y,cv=sk)\n",
    "model = xgb.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "print accuracy_score(y_test,pred)\n",
    "print roc_auc_score(y_test,pred)\n",
    "print classification_report(y_test,pred)\n",
    "print confusion_matrix(y_test,pred)\n",
    "print score\n",
    "print score.mean()\n",
    "print score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src='assets/pics/xgb.png'>\n",
    "\n",
    "Xgboost pushes the accuracy up to 76% whihc is a 3% improvement against normal Logistic Regression. One thing that Kagglers often uses in their models are ensembling. In our situation it is possible to use Sklearn to perfomr a simple voting ensemble of different models to try obtaining an even better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.voting_classifier import VotingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "skk = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = BernoulliNB()\n",
    "clf4 = KNeighborsClassifier()\n",
    "clf5 = AdaBoostClassifier()\n",
    "clf6 = ExtraTreesClassifier()\n",
    "clf7 = GradientBoostingClassifier()\n",
    "clf8 = XGBClassifier()\n",
    "\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('bnb', clf3), ('xgb',clf8),\n",
    "                                    ('adb', clf5), ('et', clf6), ('gb', clf7)], voting='soft')\n",
    "\n",
    "scr = cross_val_score(eclf,X,y,cv=skk,scoring='roc_auc')\n",
    "sca = cross_val_score(eclf,X,y,cv=skk)\n",
    "print 'CV ROC: ',scr.mean()\n",
    "print 'CV accuracy: ',sca.mean()\n",
    "\n",
    "model = eclf.fit(X_train,y_train)\n",
    "epred = model.predict(X_test)\n",
    "print accuracy_score(y_test,epred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/pics/vote.png'>\n",
    "<img src='assets/pics/voting_roc.png'>\n",
    "\n",
    "The above figures suggest that apparently this ensemble can further improve the accuracy up to over 77%. However this might not always be the case as it could be a result of random tree based models getting lucky and obtaining a good result. However in general if an ensemble of models with low correlation were used, it would usually be able to improve results further. This is because the low correlation implies the models were good at different aspects of the task and when combined should produce a better overall result. It should also be noted that the combining method plays a very important role as well. In our case, a 'soft' (weighted) voting method was used. However for a more complicated model, ensembles can be combined by a second layer model which is fitted by the output of the first layer models. This method is called stacking and is made well-known by the Netflix recommendation challenge. Plotting a ROC curve allows potential investors to gauge decision according to their risk appertite.\n",
    "\n",
    "#### Beating the Market Conclusion<a class=\"anchor\" id=\"Conclusion B\"></a>\n",
    "\n",
    "It is decided that only box sets would be investigated in this exercise as they are the most relevant forms of Lego sets and can effectively reduce the imapact of outliers. It is found out that if one only considers the numerical value of Lego sets current price, he would find that 97% of the Lego would seem to have appreciated. However one should always take into account of the variance of the current selling prices and the number listed (sample size). Hence hypotheis test should be carried out to make sure the price difference is significant. With the threshold set at 19%, almost half the number of sets have beaten the market (52%). It also appeared that one can classify those market beaters at an accuracy of over 70%. If further enhancement in accuracy was required, one can look into a simple packaged ensemble such as Xgboost. On the other hand one could custom make his own ensemble from various models using the voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps <a class=\"anchor\" id=\"Next Steps\"></a>\n",
    "\n",
    "From the above report we can consider that the initial models were quite successful. However we should also note that these successes can be highly correlated with the data structure. If we recall the correlations that we discovered in the initial analysis, we know that number of piece and MSRP is highly correlated. The situation for the classification problem is also very similar. Therefore it is not too surprising that a good model can be built. To further advance our analysis the following paths could be take:\n",
    "\n",
    "- Redefine how desirable a Lego set is by choosing other metrics as described in EDA and repeat the classification analysis\n",
    "\n",
    "- Refine both the classification and regression models. More techniques can be applied for comparison such as LDA, feature selection (e.g. KBest and RFECV). Better fine tune ensemble models such as individual components in the voting classifier\n",
    "\n",
    "- Discussion on steps to create production code from this work. It is very important to know the future peroformance of the models facing unseen data. This is already safeguarded by using cross validation. However information can still leak when model tuning takes place. Hence a nested Grid Search CV would be a better solution for such dilemma\n",
    "\n",
    "- The predictions carried out here in the initial phase might not be the most useful ones. This is because we have included many features that are available only after the set is released. For example number of reviews and 'wanted'/'owned' counts. This does not really help collectors to determine a set's value early on. Therfore to mitigate this problem, we might want to exclude the latent variables and perform the predictions solely by the other features which do not include much human influence (e.g. number of piece, themes).\n",
    "\n",
    "- There is another data set that contains information relating Lego sets with their parts. This is a large data frame and would introduce challenges in terms of high dimensionality. An analysis could be carried out to try figuring out which Lego pieces drive the price or the desirable-ness of a Lego set.\n",
    "\n",
    "- It is possible for this project to extend to some more advanced techniques such as image recognition using deep neural networks. If the image recognition is successful, a recommendation system can be created to provide build ideas to users when given a picture of various Lego pieces. This technology challenging but it would provide tremendous values because a similar idea can be applied in many other situations. It would also be a great demonstration in the final presentation as this can be made into an interactive application.\n",
    "\n",
    "\n",
    "The paths mentioned above are the more obvious suggesstions and the development of the project should not be limited to those options. However a progressing plan should be formed in order to give a clear direction of advancement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
